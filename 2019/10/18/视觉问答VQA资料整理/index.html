<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content=""><title>视觉问答VQA资料整理 | hhhhhxh</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/8.0.1/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/3.4.0/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = 'https://hm.baidu.com/hm.js?' + '7fa17d1076a1ae6cf7ec6cc0ad18ad46';
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
  })();</script></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">视觉问答VQA资料整理</h1><a id="logo" href="/.">hhhhhxh</a><p class="description"></p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a><a href="/photos/"><i class="fa fa-instagram"> 照片墙</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">视觉问答VQA资料整理</h1><div class="post-meta">Oct 18, 2019<script src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> 阅读</span></span></div><a class="disqus-comment-count" href="/2019/10/18/%E8%A7%86%E8%A7%89%E9%97%AE%E7%AD%94VQA%E8%B5%84%E6%96%99%E6%95%B4%E7%90%86/#vcomment"><span class="valine-comment-count" data-xid="/2019/10/18/%E8%A7%86%E8%A7%89%E9%97%AE%E7%AD%94VQA%E8%B5%84%E6%96%99%E6%95%B4%E7%90%86/"></span><span> 条评论</span></a><div class="clear"><div class="toc-article" id="toc"><div class="toc-title">文章目录</div><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#入门学习"><span class="toc-number">1.</span> <span class="toc-text">入门学习</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#进阶论文"><span class="toc-number">2.</span> <span class="toc-text">进阶论文</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Normal"><span class="toc-number">2.1.</span> <span class="toc-text">Normal</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Attention-Based"><span class="toc-number">2.2.</span> <span class="toc-text">Attention-Based</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Knowledge-based"><span class="toc-number">2.3.</span> <span class="toc-text">Knowledge-based</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Memory-Network"><span class="toc-number">2.4.</span> <span class="toc-text">Memory Network</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Video-QA"><span class="toc-number">2.5.</span> <span class="toc-text">Video QA</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#综述"><span class="toc-number">3.</span> <span class="toc-text">综述</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Tutorial"><span class="toc-number">4.</span> <span class="toc-text">Tutorial</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Dataset"><span class="toc-number">5.</span> <span class="toc-text">Dataset</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Code"><span class="toc-number">6.</span> <span class="toc-text">Code</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#领域专家"><span class="toc-number">7.</span> <span class="toc-text">领域专家</span></a></li></ol></div></div><div class="post-content"><a id="more"></a>
<h1 id="入门学习"><a href="#入门学习" class="headerlink" title="入门学习"></a>入门学习</h1><ol>
<li><a href="https://zhuanlan.zhihu.com/p/22530291" target="_blank" rel="noopener">基于深度学习的VQA(视觉问答)技术</a></li>
<li><a href="https://mp.weixin.qq.com/s/dyor9bv2y0VyX7woMDVLkA" target="_blank" rel="noopener">视觉问答全景概述：从数据集到技术方法</a></li>
<li><a href="http://www.jianshu.com/p/5bf03d1fadfa" target="_blank" rel="noopener">论文读书笔记(Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding)</a></li>
<li><a href="https://www.leiphone.com/news/201711/4B9cNlCINsVyPdTw.html" target="_blank" rel="noopener">能看图回答问题的AI离我们还有多远？Facebook向视觉对话进发</a></li>
<li><a href="http://www.cnblogs.com/ranjiewen/p/7604468.html" target="_blank" rel="noopener">图像问答Image Question Answering</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/20899091" target="_blank" rel="noopener">实战深度学习之图像问答</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/29688475" target="_blank" rel="noopener">2017 VQA Challenge 第一名技术报告</a></li>
<li><a href="http://www.msra.cn/zh-cn/news/features/vision-and-language-20170713" target="_blank" rel="noopener">深度学习为视觉和语言之间搭建了一座桥梁</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/59530688" target="_blank" rel="noopener">VQA相关方法的简单综述</a></li>
<li><a href="https://www.jianshu.com/p/76d2e081e303" target="_blank" rel="noopener">一文带你了解视觉问答VQA</a></li>
</ol>
<h1 id="进阶论文"><a href="#进阶论文" class="headerlink" title="进阶论文"></a>进阶论文</h1><h2 id="Normal"><a href="#Normal" class="headerlink" title="Normal"></a>Normal</h2><ol>
<li><p><a href="https://arxiv.org/abs/1610.01465" target="_blank" rel="noopener">Kushal Kafle, and Christopher Kanan.Visual question answering: Datasets, algorithms, and future challenges.Computer Vision and Image Understanding [2017]</a></p>
</li>
<li><p><a href="http://vision.stanford.edu/pdf/johnson2017cvpr.pdf" target="_blank" rel="noopener">Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Li Fei-Fei, C. Lawrence Zitnick, Ross Girshick, CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning, CVPR 2017</a></p>
</li>
<li><p><a href="https://arxiv.org/abs/1705.03633" target="_blank" rel="noopener">Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Judy Hoffman, Li Fei-Fei, C. Lawrence Zitnick, Ross Girshick, Inferring and Executing Programs for Visual Reasoning, arXiv:1705.03633, 2017.</a></p>
</li>
<li><p><a href="https://arxiv.org/abs/1704.05526" target="_blank" rel="noopener">Ronghang Hu, Jacob Andreas, Marcus Rohrbach, Trevor Darrell, Kate Saenko, Learning to Reason: End-to-End Module Networks for Visual Question Answering, arXiv:1704.05526, 2017. </a></p>
</li>
<li><p><a href="https://arxiv.org/abs/1706.01427" target="_blank" rel="noopener">Adam Santoro, David Raposo, David G.T. Barrett, Mateusz Malinowski, Razvan Pascanu, Peter Battaglia, Timothy Lillicrap, A simple neural network module for relational reasoning, arXiv:1706.01427, 2017.</a></p>
</li>
<li><p><a href="https://arxiv.org/pdf/1705.06676.pdf" target="_blank" rel="noopener">Hedi Ben-younes, Remi Cadene, Matthieu Cord, Nicolas Thome: MUTAN: Multimodal Tucker Fusion for Visual Question Answering</a></p>
<p>github: <a href="https://github.com/Cadene/vqa.pytorch" target="_blank" rel="noopener">https://github.com/Cadene/vqa.pytorch</a></p>
</li>
<li><p><a href="https://arxiv.org/abs/1704.03162" target="_blank" rel="noopener">Vahid Kazemi, Ali Elqursh, Show, Ask, Attend, and Answer: A Strong Baseline For Visual Question Answering, arXiv:1704.03162, 2016.</a></p>
<p>github: <a href="https://github.com/Cyanogenoid/pytorch-vqa" target="_blank" rel="noopener">https://github.com/Cyanogenoid/pytorch-vqa</a></p>
</li>
<li><p><a href="https://arxiv.org/abs/1703.09684" target="_blank" rel="noopener">Kushal Kafle, and Christopher Kanan. <em>An Analysis of Visual Question Answering Algorithms.</em> arXiv:1703.09684, 2017.</a></p>
</li>
<li><p><a href="https://arxiv.org/abs/1611.00471" target="_blank" rel="noopener">Hyeonseob Nam, Jung-Woo Ha, Jeonghee Kim, Dual Attention Networks for Multimodal Reasoning and Matching, arXiv:1611.00471, 2016.</a></p>
</li>
<li><p><a href="https://arxiv.org/abs/1610.04325" target="_blank" rel="noopener">Jin-Hwa Kim, Kyoung Woon On, Jeonghee Kim, Jung-Woo Ha, Byoung-Tak Zhang, Hadamard Product for Low-rank Bilinear Pooling, arXiv:1610.04325, 2016.</a></p>
</li>
<li><p><a href="https://arxiv.org/abs/1606.01847" target="_blank" rel="noopener">Akira Fukui, Dong Huk Park, Daylen Yang, Anna Rohrbach, Trevor Darrell, Marcus Rohrbach, Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding, arXiv:1606.01847, 2016.</a></p>
<p>github: <a href="https://github.com/akirafukui/vqa-mcb" target="_blank" rel="noopener">https://github.com/akirafukui/vqa-mcb</a></p>
</li>
<li><p><a href="https://arxiv.org/pdf/1606.06108.pdf" target="_blank" rel="noopener">Kuniaki Saito, Andrew Shin, Yoshitaka Ushiku, Tatsuya Harada, DualNet: Domain-Invariant Network for Visual Question Answering. arXiv:1606.06108v1, 2016.</a></p>
</li>
<li><p><a href="https://arxiv.org/pdf/1606.06622v1.pdf" target="_blank" rel="noopener">Arijit Ray, Gordon Christie, Mohit Bansal, Dhruv Batra, Devi Parikh, Question Relevance in VQA: Identifying Non-Visual And False-Premise Questions, arXiv:1606.06622, 2016.</a></p>
</li>
<li><p><a href="http://arxiv.org/abs/1606.03647v1" target="_blank" rel="noopener">Hyeonwoo Noh, Bohyung Han, Training Recurrent Answering Units with Joint Loss Minimization for VQA, arXiv:1606.03647, 2016.</a></p>
</li>
<li><p><a href="https://arxiv.org/pdf/1606.00061v2.pdf" target="_blank" rel="noopener">Jiasen Lu, Jianwei Yang, Dhruv Batra, Devi Parikh, Hierarchical Question-Image Co-Attention for Visual Question Answering, arXiv:1606.00061, 2016.</a></p>
<p>github: <a href="https://github.com/jiasenlu/HieCoAttenVQA" target="_blank" rel="noopener">https://github.com/jiasenlu/HieCoAttenVQA</a></p>
</li>
<li><p><a href="https://arxiv.org/pdf/1606.01455v1.pdf" target="_blank" rel="noopener">Jin-Hwa Kim, Sang-Woo Lee, Dong-Hyun Kwak, Min-Oh Heo, Jeonghee Kim, Jung-Woo Ha, Byoung-Tak Zhang, Multimodal Residual Learning for Visual QA, arXiv:1606.01455, 2016.</a></p>
</li>
<li><p><a href="https://arxiv.org/pdf/1606.05433.pdf" target="_blank" rel="noopener">Peng Wang, Qi Wu, Chunhua Shen, Anton van den Hengel, Anthony Dick, FVQA: Fact-based Visual Question Answering, arXiv:1606.05433, 2016.</a></p>
</li>
<li><p><a href="https://arxiv.org/pdf/1604.01485v1.pdf" target="_blank" rel="noopener">Ilija Ilievski, Shuicheng Yan, Jiashi Feng, A Focused Dynamic Attention Model for Visual Question Answering, arXiv:1604.01485.</a></p>
</li>
<li><p><a href="http://arxiv.org/abs/1511.03416" target="_blank" rel="noopener">Yuke Zhu, Oliver Groth, Michael Bernstein, Li Fei-Fei, Visual7W: Grounded Question Answering in Images, CVPR 2016.</a></p>
</li>
<li><p><a href="http://arxiv.org/pdf/1511.05756.pdf" target="_blank" rel="noopener">Hyeonwoo Noh, Paul Hongsuck Seo, and Bohyung Han, Image Question Answering using Convolutional Neural Network with Dynamic Parameter Prediction, CVPR, 2016.</a></p>
</li>
<li><p><a href="http://arxiv.org/pdf/1601.01705.pdf" target="_blank" rel="noopener">Jacob Andreas, Marcus Rohrbach, Trevor Darrell, Dan Klein, Learning to Compose Neural Networks for Question Answering, NAACL 2016.</a></p>
</li>
<li><p><a href="https://arxiv.org/abs/1511.02799" target="_blank" rel="noopener">Jacob Andreas, Marcus Rohrbach, Trevor Darrell, Dan Klein, Deep compositional question answering with neural module networks, CVPR 2016.</a></p>
</li>
<li><p><a href="http://arxiv.org/abs/1511.02274" target="_blank" rel="noopener">Zichao Yang, Xiaodong He, Jianfeng Gao, Li Deng, Alex Smola, Stacked Attention Networks for Image Question Answering, CVPR 2016.</a></p>
<p>github: <a href="https://github.com/JamesChuanggg/san-torch" target="_blank" rel="noopener">https://github.com/JamesChuanggg/san-torch</a></p>
</li>
<li><p><a href="http://arxiv.org/pdf/1511.07394v2.pdf" target="_blank" rel="noopener">Kevin J. Shih, Saurabh Singh, Derek Hoiem, Where To Look: Focus Regions for Visual Question Answering, CVPR, 2015.</a></p>
</li>
<li><p><a href="http://arxiv.org/pdf/1511.05960v1.pdf" target="_blank" rel="noopener">Kan Chen, Jiang Wang, Liang-Chieh Chen, Haoyuan Gao, Wei Xu, Ram Nevatia, ABC-CNN: An Attention Based Convolutional Neural Network for Visual Question Answering, arXiv:1511.05960v1, Nov 2015.</a></p>
</li>
<li><p><a href="http://arxiv.org/abs/1511.05234" target="_blank" rel="noopener">Huijuan Xu, Kate Saenko, Ask, Attend and Answer: Exploring Question-Guided Spatial Attention for Visual Question Answering, arXiv:1511.05234v1, Nov 2015.</a></p>
</li>
<li><p><a href="http://www.cv-foundation.org/openaccess/content_cvpr_2016/html/Kafle_Answer-Type_Prediction_for_CVPR_2016_paper.html" target="_blank" rel="noopener">Kushal Kafle and Christopher Kanan, Answer-Type Prediction for Visual Question Answering, CVPR 2016.</a></p>
</li>
<li><p><a href="http://arxiv.org/pdf/1505.00468" target="_blank" rel="noopener">Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C. Lawrence Zitnick, Devi Parikh, VQA: Visual Question Answering, ICCV, 2015.</a></p>
</li>
<li><p><a href="http://arxiv.org/pdf/1505.00468" target="_blank" rel="noopener">Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C. Lawrence Zitnick, Devi Parikh, VQA: Visual Question Answering, ICCV, 2015.</a> </p>
<p>github: <a href="https://github.com/JamesChuanggg/VQA-tensorflow" target="_blank" rel="noopener">https://github.com/JamesChuanggg/VQA-tensorflow</a></p>
</li>
<li><p><a href="http://arxiv.org/abs/1512.02167" target="_blank" rel="noopener">Bolei Zhou, Yuandong Tian, Sainbayar Sukhbaatar, Arthur Szlam, Rob Fergus, Simple Baseline for Visual Question Answering, arXiv:1512.02167v2, Dec 2015.</a></p>
</li>
<li><p><a href="http://arxiv.org/pdf/1505.05612.pdf" target="_blank" rel="noopener">Hauyuan Gao, Junhua Mao, Jie Zhou, Zhiheng Huang, Lei Wang, Wei Xu, Are You Talking to a Machine? Dataset and Methods for Multilingual Image Question Answering, NIPS 2015.</a></p>
</li>
<li><p><a href="http://arxiv.org/pdf/1505.01121v3.pdf" target="_blank" rel="noopener">Mateusz Malinowski, Marcus Rohrbach, Mario Fritz, Ask Your Neurons: A Neural-based Approach to Answering Questions about Images, ICCV 2015.</a></p>
</li>
<li><p><a href="http://arxiv.org/pdf/1505.02074.pdf" target="_blank" rel="noopener">Mengye Ren, Ryan Kiros, Richard Zemel, Exploring Models and Data for Image Question Answering, ICML 2015.</a></p>
</li>
<li><p><a href="http://arxiv.org/abs/1410.8027" target="_blank" rel="noopener">Mateusz Malinowski, Mario Fritz, Towards a Visual Turing Challe, NIPS Workshop 2015.</a></p>
</li>
<li><p><a href="http://arxiv.org/pdf/1410.0210v4.pdf" target="_blank" rel="noopener">Mateusz Malinowski, Mario Fritz, A Multi-World Approach to Question Answering about Real-World Scenes based on Uncertain Input, NIPS 2014.</a></p>
</li>
</ol>
<h2 id="Attention-Based"><a href="#Attention-Based" class="headerlink" title="Attention-Based"></a>Attention-Based</h2><ol>
<li><p><a href="https://arxiv.org/pdf/1705.06676.pdf" target="_blank" rel="noopener">Hedi Ben-younes, Remi Cadene, Matthieu Cord, Nicolas Thome: MUTAN: Multimodal Tucker Fusion for Visual Question Answering</a></p>
<p>github: <a href="https://github.com/Cadene/vqa.pytorch" target="_blank" rel="noopener">https://github.com/Cadene/vqa.pytorch</a></p>
</li>
<li><p><a href="https://arxiv.org/abs/1610.04325" target="_blank" rel="noopener">Jin-Hwa Kim, Kyoung Woon On, Jeonghee Kim, Jung-Woo Ha, Byoung-Tak Zhang, Hadamard Product for Low-rank Bilinear Pooling, arXiv:1610.04325, 2016.</a></p>
</li>
<li><p><a href="https://arxiv.org/abs/1606.01847" target="_blank" rel="noopener">Akira Fukui, Dong Huk Park, Daylen Yang, Anna Rohrbach, Trevor Darrell, Marcus Rohrbach, Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding, arXiv:1606.01847, 2016.</a></p>
</li>
<li><p><a href="http://arxiv.org/abs/1606.03647v1" target="_blank" rel="noopener">Hyeonwoo Noh, Bohyung Han, Training Recurrent Answering Units with Joint Loss Minimization for VQA, arXiv:1606.03647, 2016.</a></p>
</li>
<li><p><a href="https://arxiv.org/pdf/1606.00061v2.pdf" target="_blank" rel="noopener">Jiasen Lu, Jianwei Yang, Dhruv Batra, Devi Parikh, Hierarchical Question-Image Co-Attention for Visual Question Answering, arXiv:1606.00061, 2016.</a></p>
</li>
<li><p><a href="http://arxiv.org/abs/1511.02274" target="_blank" rel="noopener">Zichao Yang, Xiaodong He, Jianfeng Gao, Li Deng, Alex Smola, Stacked Attention Networks for Image Question Answering, CVPR 2016.</a></p>
</li>
<li><p><a href="https://arxiv.org/pdf/1604.01485v1.pdf" target="_blank" rel="noopener">Ilija Ilievski, Shuicheng Yan, Jiashi Feng, A Focused Dynamic Attention Model for Visual Question Answering, arXiv:1604.01485.</a></p>
</li>
<li><p><a href="http://arxiv.org/pdf/1511.05960v1.pdf" target="_blank" rel="noopener">Kan Chen, Jiang Wang, Liang-Chieh Chen, Haoyuan Gao, Wei Xu, Ram Nevatia, ABC-CNN: An Attention Based Convolutional Neural Network for Visual Question Answering, arXiv:1511.05960v1, Nov 2015.</a></p>
</li>
<li><p><a href="http://arxiv.org/abs/1511.05234" target="_blank" rel="noopener">Huijuan Xu, Kate Saenko, Ask, Attend and Answer: Exploring Question-Guided Spatial Attention for Visual Question Answering, arXiv:1511.05234v1, Nov 2015.</a></p>
</li>
</ol>
<h2 id="Knowledge-based"><a href="#Knowledge-based" class="headerlink" title="Knowledge-based"></a>Knowledge-based</h2><ol>
<li><a href="https://arxiv.org/pdf/1606.05433.pdf" target="_blank" rel="noopener">Peng Wang, Qi Wu, Chunhua Shen, Anton van den Hengel, Anthony Dick, FVQA: Fact-based Visual Question Answering, arXiv:1606.05433, 2016.</a></li>
<li><a href="http://arxiv.org/abs/1511.06973" target="_blank" rel="noopener">Qi Wu, Peng Wang, Chunhua Shen, Anton van den Hengel, Anthony Dick, Ask Me Anything: Free-form Visual Question Answering Based on Knowledge from External Sources, CVPR 2016.</a></li>
<li><a href="http://arxiv.org/abs/1511.02570" target="_blank" rel="noopener">Peng Wang, Qi Wu, Chunhua Shen, Anton van den Hengel, Anthony Dick, Explicit Knowledge-based Reasoning for Visual Question Answering, arXiv:1511.02570v2, Nov 2015.</a></li>
<li><a href="http://arxiv.org/abs/1507.05670" target="_blank" rel="noopener">Yuke Zhu, Ce Zhang, Christopher Re,́ Li Fei-Fei, Building a Large-scale Multimodal Knowledge Base System for Answering Visual Queries, arXiv:1507.05670, Nov 2015.</a></li>
</ol>
<h2 id="Memory-Network"><a href="#Memory-Network" class="headerlink" title="Memory Network"></a>Memory Network</h2><ol>
<li><a href="http://arxiv.org/abs/1603.01417" target="_blank" rel="noopener">Caiming Xiong, Stephen Merity, Richard Socher, Dynamic Memory Networks for Visual and Textual Question Answering, ICML 2016.</a></li>
<li><a href="http://arxiv.org/abs/1511.05676" target="_blank" rel="noopener">Aiwen Jiang, Fang Wang, Fatih Porikli, Yi Li, Compositional Memory for Visual Question Answering, arXiv:1511.05676v1, Nov 2015.</a></li>
</ol>
<h2 id="Video-QA"><a href="#Video-QA" class="headerlink" title="Video QA"></a>Video QA</h2><ol>
<li><a href="https://arxiv.org/abs/1611.04021" target="_blank" rel="noopener">Kuo-Hao Zeng, Tseng-Hung Chen, Ching-Yao Chuang, Yuan-Hong Liao, Juan Carlos Niebles, Min Sun, Leveraging Video Descriptions to Learn Video Question Answering, AAAI 2017.</a></li>
<li><a href="http://arxiv.org/abs/1512.02902" target="_blank" rel="noopener">Makarand Tapaswi, Yukun Zhu, Rainer Stiefelhagen, Antonio Torralba, Raquel Urtasun, Sanja Fidler, MovieQA: Understanding Stories in Movies through Question-Answering, CVPR 2016.</a></li>
<li><a href="http://arxiv.org/abs/1511.04670" target="_blank" rel="noopener">Linchao Zhu, Zhongwen Xu, Yi Yang, Alexander G. Hauptmann, Uncovering Temporal Context for Video Question and Answering, arXiv:1511.05676v1, Nov 2015.</a></li>
</ol>
<h1 id="综述"><a href="#综述" class="headerlink" title="综述"></a>综述</h1><ol>
<li><a href="https://arxiv.org/abs/1607.05910" target="_blank" rel="noopener">Qi Wu, Damien Teney, Peng Wang, Chunhua Shen, Anthony Dick, and Anton van den Hengel. Visual question answering: A survey of methods and datasets. Computer Vision and Image Understanding [2017].</a></li>
<li><a href="https://arxiv.org/abs/1610.01076" target="_blank" rel="noopener">Tutorial on Answering Questions about Images with Deep Learning Mateusz Malinowski, Mario Fritz</a></li>
<li><a href="https://arxiv.org/abs/1705.03865" target="_blank" rel="noopener">Survey of Visual Question Answering: Datasets and Techniques</a></li>
<li><a href="https://arxiv.org/abs/1610.01465" target="_blank" rel="noopener">Visual Question Answering: Datasets, Algorithms, and Future Challenges</a></li>
</ol>
<h1 id="Tutorial"><a href="#Tutorial" class="headerlink" title="Tutorial"></a>Tutorial</h1><ol>
<li><a href="http://www.visualqa.org/workshop.html" target="_blank" rel="noopener">CVPR 2017 VQA Challenge Workshop(有很多PPT)</a></li>
<li><a href="http://www.visualqa.org/vqa_v1_workshop.html" target="_blank" rel="noopener">CVPR 2016 VQA Challenge Workshop</a></li>
<li><a href="https://arxiv.org/pdf/1610.01076.pdf" target="_blank" rel="noopener">Tutorial on Answering Questions about Images with Deep Learning</a></li>
<li><a href="http://iamaaditya.github.io/2016/04/visual_question_answering_demo_notebook" target="_blank" rel="noopener">Visual Question Answering Demo in Python Notebook</a></li>
<li><a href="https://www.linkedin.com/pulse/tutorial-question-answering-images-mateusz-malinowski" target="_blank" rel="noopener">Tutorial on Question Answering about Images</a></li>
</ol>
<h1 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h1><ol>
<li><p>Visual7W: Grounded Question Answering in Images</p>
<ul>
<li>homepage: <a href="http://web.stanford.edu/~yukez/visual7w/" target="_blank" rel="noopener">http://web.stanford.edu/~yukez/visual7w/</a></li>
<li>github: <a href="https://github.com/yukezhu/visual7w-toolkit" target="_blank" rel="noopener">https://github.com/yukezhu/visual7w-toolkit</a></li>
<li>github: <a href="https://github.com/yukezhu/visual7w-qa-models" target="_blank" rel="noopener">https://github.com/yukezhu/visual7w-qa-models</a></li>
</ul>
</li>
<li><p>DAQUAR</p>
<ul>
<li><a href="http://www.cs.toronto.edu/~mren/imageqa/results/" target="_blank" rel="noopener">http://www.cs.toronto.edu/~mren/imageqa/results/</a></li>
</ul>
</li>
<li><p>COCO-QA</p>
<ul>
<li><a href="http://www.cs.toronto.edu/~mren/imageqa/data/cocoqa/" target="_blank" rel="noopener">http://www.cs.toronto.edu/~mren/imageqa/data/cocoqa/</a></li>
</ul>
</li>
<li><p>The VQA Dataset</p>
<ul>
<li><a href="http://visualqa.org/" target="_blank" rel="noopener">http://visualqa.org/</a></li>
</ul>
</li>
<li><p>FM-IQA</p>
<ul>
<li><a href="http://idl.baidu.com/FM-IQA.html" target="_blank" rel="noopener">http://idl.baidu.com/FM-IQA.html</a></li>
</ul>
</li>
<li>Visual Genome<ul>
<li><a href="http://visualgenome.org/" target="_blank" rel="noopener">http://visualgenome.org/</a></li>
</ul>
</li>
</ol>
<h1 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h1><ol>
<li><p>VQA Demo: Visual Question Answering Demo on pretrained model</p>
<ul>
<li><p>[<a href="https://github.com/iamaaditya/VQA_Demo" target="_blank" rel="noopener">https://github.com/iamaaditya/VQA_Demo</a>]</p>
</li>
<li><p>[<a href="http://iamaaditya.github.io/research/" target="_blank" rel="noopener">http://iamaaditya.github.io/research/</a>]</p>
</li>
</ul>
</li>
<li><p>deep-qa: Implementation of the Convolution Neural Network for factoid QA on the answer sentence selection task</p>
<ul>
<li>[<a href="https://github.com/aseveryn/deep-qa" target="_blank" rel="noopener">https://github.com/aseveryn/deep-qa</a>]</li>
</ul>
</li>
<li><p>YodaQA: A Question Answering system built on top of the Apache UIMA framework</p>
<ul>
<li><p>[<a href="http://ailao.eu/yodaqa/" target="_blank" rel="noopener">http://ailao.eu/yodaqa/</a>]</p>
</li>
<li><p>[<a href="https://github.com/brmson/yodaqa" target="_blank" rel="noopener">https://github.com/brmson/yodaqa</a>]</p>
</li>
</ul>
</li>
<li><p>insuranceQA-cnn-lstm: tensorflow and theano cnn code for insurance QA</p>
<ul>
<li>[<a href="https://github.com/white127/insuranceQA-cnn-lstm" target="_blank" rel="noopener">https://github.com/white127/insuranceQA-cnn-lstm</a>]</li>
</ul>
</li>
<li><p>Tensorflow Implementation of Deeper LSTM+ normalized CNN for Visual Question Answering</p>
<ul>
<li>[<a href="https://github.com/JamesChuanggg/VQA-tensorflow" target="_blank" rel="noopener">https://github.com/JamesChuanggg/VQA-tensorflow</a>]</li>
</ul>
</li>
<li><p>Visual Question Answering with Keras</p>
<ul>
<li>[<a href="https://anantzoid.github.io/VQA-Keras-Visual-Question-Answering/" target="_blank" rel="noopener">https://anantzoid.github.io/VQA-Keras-Visual-Question-Answering/</a>]</li>
</ul>
</li>
<li><p>Deep Learning Models for Question Answering with Keras</p>
<ul>
<li>[<a href="http://sujitpal.blogspot.jp/2016/10/deep-learning-models-for-question.html" target="_blank" rel="noopener">http://sujitpal.blogspot.jp/2016/10/deep-learning-models-for-question.html</a>]</li>
</ul>
</li>
<li><p>Deep QA: Using deep learning to answer Aristo’s science questions</p>
<ul>
<li>[<a href="https://github.com/allenai/deep_qa" target="_blank" rel="noopener">https://github.com/allenai/deep_qa</a>]</li>
</ul>
</li>
<li><p>Visual Question Answering in Pytorch</p>
<ul>
<li>[<a href="https://github.com/Cadene/vqa.pytorch" target="_blank" rel="noopener">https://github.com/Cadene/vqa.pytorch</a>]</li>
</ul>
</li>
</ol>
<h1 id="领域专家"><a href="#领域专家" class="headerlink" title="领域专家"></a>领域专家</h1><ul>
<li>Qi Wu<ul>
<li>[<a href="https://researchers.adelaide.edu.au/profile/qi.wu01" target="_blank" rel="noopener">https://researchers.adelaide.edu.au/profile/qi.wu01</a>]</li>
</ul>
</li>
<li>Bolei Zhou 周博磊<ul>
<li>[<a href="http://people.csail.mit.edu/bzhou/" target="_blank" rel="noopener">http://people.csail.mit.edu/bzhou/</a>]</li>
</ul>
</li>
<li>Stanislaw Antol<ul>
<li>[<a href="https://computing.ece.vt.edu/~santol/\" target="_blank" rel="noopener">https://computing.ece.vt.edu/~santol/\</a>]</li>
</ul>
</li>
<li>Jin-Hwa Kim<ul>
<li>[<a href="https://bi.snu.ac.kr/~jhkim/\" target="_blank" rel="noopener">https://bi.snu.ac.kr/~jhkim/\</a>]</li>
</ul>
</li>
<li>Vahid Kazemi<ul>
<li>[<a href="http://www.csc.kth.se/~vahidk/index.html\" target="_blank" rel="noopener">http://www.csc.kth.se/~vahidk/index.html\</a>]</li>
</ul>
</li>
<li>Justin Johnson<ul>
<li>[<a href="http://cs.stanford.edu/people/jcjohns/" target="_blank" rel="noopener">http://cs.stanford.edu/people/jcjohns/</a>]</li>
</ul>
</li>
<li>Ilija Ilievski<ul>
<li>[<a href="https://ilija139.github.io/" target="_blank" rel="noopener">https://ilija139.github.io/</a>]</li>
</ul>
</li>
</ul>
</div><div class="tags"><a href="/tags/%E8%B5%84%E6%96%99%E6%95%B4%E7%90%86/">资料整理</a></div><div class="post-nav"><a class="pre" href="/2019/10/22/%E9%9B%B6%E6%A0%B7%E6%9C%AC%E5%AD%A6%E4%B9%A0ZSL%E8%B5%84%E6%96%99%E6%95%B4%E7%90%86/">零样本学习ZSL资料整理</a><a class="next" href="/2019/10/07/hello-world/">Hello World</a></div><div id="vcomment"></div><script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script><script src="//unpkg.com/valine@latest/dist/Valine.min.js"></script><script>var notify = 'false' ? true : false;
var verify = 'false' ? true : false;
var GUEST_INFO = ['nick','mail','link'];
var guest_info = 'nick,mail,link'.split(',').filter(function(item){
  return GUEST_INFO.indexOf(item) > -1
});
guest_info = guest_info.length == 0 ? GUEST_INFO :guest_info;
window.valine = new Valine({
  el:'#vcomment',
  notify:notify,
  verify:verify,
  appId:'OSUK7P1Utb45xOOhM0JJbTab-gzGzoHsz',
  appKey:'0OAn3KMQOEbFWSQB5P17tM7h',
  placeholder:'Just so so',
  avatar:'mm',
  guest_info:guest_info,
  pageSize:'10'
})</script></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank"><input type="text" name="q" maxlength="20" placeholder="Search"/><input type="hidden" name="sitesearch" value="http://hhhhhxh.github.io"/></form></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/tutorial/" style="font-size: 15px;">tutorial</a> <a href="/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/" style="font-size: 15px;">论文笔记</a> <a href="/tags/%E8%B5%84%E6%96%99%E6%95%B4%E7%90%86/" style="font-size: 15px;">资料整理</a> <a href="/tags/%E9%B8%A1%E6%B1%A4/" style="font-size: 15px;">鸡汤</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2020/01/01/2020-Flag/">2020 Flag</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/12/13/AdaGrad-RMSProp-Adam%E7%AE%97%E6%B3%95%E7%9A%84%E7%AE%80%E5%8D%95%E7%90%86%E8%A7%A3/">AdaGrad,RMSProp,Adam算法的简单理解</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/11/27/%E5%AF%B9%E9%9B%B6%E6%A0%B7%E6%9C%AC%E5%AD%A6%E4%B9%A0%E4%B8%AD%E6%BD%9C%E5%9C%A8%E7%89%B9%E5%BE%81%E7%9A%84%E5%88%A4%E5%88%AB%E5%AD%A6%E4%B9%A0/">对零样本学习中潜在特征的判别学习</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/11/14/%E4%B8%80%E7%AF%87%E9%9B%B6%E6%A0%B7%E6%9C%AC%E5%AD%A6%E4%B9%A0%E7%9A%84%E7%BB%BC%E8%BF%B0%E2%80%94%E2%80%94%E9%92%88%E5%AF%B9%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E6%8B%86%E5%88%86/">一篇零样本学习的综述——针对数据集的拆分</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/11/12/SEVER%EF%BC%8C%E4%B8%80%E7%A7%8D%E6%A2%AF%E5%BA%A6%E4%BC%98%E5%8C%96%E7%9A%84%E9%B2%81%E6%A3%92%E5%85%83%E7%AE%97%E6%B3%95/">SEVER——一种梯度优化的鲁棒元算法</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/11/10/%E5%85%B1%E5%8B%89/">共勉</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/10/29/git-lfs%E7%9A%84%E4%BD%BF%E7%94%A8/">git lfs的使用</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/10/22/%E9%9B%B6%E6%A0%B7%E6%9C%AC%E5%AD%A6%E4%B9%A0ZSL%E8%B5%84%E6%96%99%E6%95%B4%E7%90%86/">零样本学习ZSL资料整理</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/10/18/%E8%A7%86%E8%A7%89%E9%97%AE%E7%AD%94VQA%E8%B5%84%E6%96%99%E6%95%B4%E7%90%86/">视觉问答VQA资料整理</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/10/07/hello-world/">Hello World</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="http://www.example1.com/" title="site-name1" target="_blank">site-name1</a><ul></ul><a href="http://www.example2.com/" title="site-name2" target="_blank">site-name2</a><ul></ul><a href="http://www.example3.com/" title="site-name3" target="_blank">site-name3</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2020 <a href="/." rel="nofollow">hhhhhxh.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css"><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>