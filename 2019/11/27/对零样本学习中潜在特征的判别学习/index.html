<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content=""><title>对零样本学习中潜在特征的判别学习 | hhhhhxh</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/8.0.1/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/3.4.0/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = 'https://hm.baidu.com/hm.js?' + '7fa17d1076a1ae6cf7ec6cc0ad18ad46';
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
  })();</script></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">对零样本学习中潜在特征的判别学习</h1><a id="logo" href="/.">hhhhhxh</a><p class="description"></p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a><a href="/photos/"><i class="fa fa-instagram"> 照片墙</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">对零样本学习中潜在特征的判别学习</h1><div class="post-meta">Nov 27, 2019<script src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> 阅读</span></span></div><a class="disqus-comment-count" href="/2019/11/27/%E5%AF%B9%E9%9B%B6%E6%A0%B7%E6%9C%AC%E5%AD%A6%E4%B9%A0%E4%B8%AD%E6%BD%9C%E5%9C%A8%E7%89%B9%E5%BE%81%E7%9A%84%E5%88%A4%E5%88%AB%E5%AD%A6%E4%B9%A0/#vcomment"><span class="valine-comment-count" data-xid="/2019/11/27/%E5%AF%B9%E9%9B%B6%E6%A0%B7%E6%9C%AC%E5%AD%A6%E4%B9%A0%E4%B8%AD%E6%BD%9C%E5%9C%A8%E7%89%B9%E5%BE%81%E7%9A%84%E5%88%A4%E5%88%AB%E5%AD%A6%E4%B9%A0/"></span><span> 条评论</span></a><div class="clear"><div class="toc-article" id="toc"><div class="toc-title">文章目录</div><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#FNet"><span class="toc-number">1.</span> <span class="toc-text">FNet</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#ZNet"><span class="toc-number">2.</span> <span class="toc-text">ZNet</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#ENet"><span class="toc-number">3.</span> <span class="toc-text">ENet</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#ZSL-Prediction"><span class="toc-number">4.</span> <span class="toc-text">ZSL Prediction</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Combining-multiple-spaces"><span class="toc-number">4.1.</span> <span class="toc-text">Combining multiple spaces</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Combining-multiple-scales"><span class="toc-number">4.2.</span> <span class="toc-text">Combining multiple scales</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#multi-scale-UA-features"><span class="toc-number">4.2.1.</span> <span class="toc-text">multi-scale UA features</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#multi-scale-LA-features"><span class="toc-number">4.2.2.</span> <span class="toc-text">multi-scale LA features</span></a></li></ol></li></ol></li></ol></div></div><div class="post-content"><p>这篇论文出自CVPR2018。作者提出了：</p>
<ol>
<li>通过zoom network从图像中提取判别性区域</li>
<li>将普通的自定义特征表示用更加具有判别性的语义表示来进行拓展</li>
</ol>
<p><img src="/images/《Discriminative-Learning-of-Latent-Features-for-Zero-Shot-Recognition》论文笔记/figure2.png" alt=""></p>
<p>网络中主要分为3个部分，FNet，ZNet和ENet</p>
<h1 id="FNet"><a href="#FNet" class="headerlink" title="FNet"></a>FNet</h1><p>The Image Feature Network，用来提取图像特征，可以用VGG19也可以用GoogLeNet</p>
<p>对于x为图像或图像的zoon region，提取到的特征为：</p>
<script type="math/tex; mode=display">\phi(x)=W_{IF}*x</script><p>其中$W_{IF}$为overall parameters</p>
<p>需要注意的是，<strong>FNet的参数是由这个框架中的几个部分共同训练的</strong></p>
<h1 id="ZNet"><a href="#ZNet" class="headerlink" title="ZNet"></a>ZNet</h1><p>The Zoom Net，提取图像中的判别性区域，说人话就是把图里的目标放大</p>
<p>目标区域被定为正方形，由3个参数指定：</p>
<script type="math/tex; mode=display">[z_x,z_y,z_s]=W_z*\phi(x)_{conv}</script><p>其中$z_x$和$z_y$指定了正方形中心点的位置，$z_s$指定了正方形的边长，$W_z$为ZNet的参数</p>
<p>确定了之后直接把目标区域从图像中裁出来就可以了，但是由于<em>在反向传播中优化非连续裁剪操作</em>不方便，所以作者提出了二维连续掩模$M(x,y)$</p>
<script type="math/tex; mode=display">M_x=f(x-z_x+0.5z_s)-f(x-z_x-0.5z_s)</script><script type="math/tex; mode=display">M_y=f(y-z_y+0.5z_s)-f(y-z_y-0.5z_s)</script><p>其中$f(x)=1/(1+exp(-kx))$，实验中$k=10$</p>
<p>这样：</p>
<script type="math/tex; mode=display">x^{crop}=x\odot M</script><h1 id="ENet"><a href="#ENet" class="headerlink" title="ENet"></a>ENet</h1><p>The Embedding Network，与前面FNet不同的是，ENet更多偏向于提取的是类的语义特征。作者认为大部分的ZSL方法对于语义的属性特征都是自定义属性，但是自定义属性可能有很多判别性明显的属性无法提取到。因此在augmented attribute space中，图片被同时映射到UA(user-defined attributes)和LA(latent discriminative attributes)中</p>
<script type="math/tex; mode=display">the \; embedding\; image\; features\; \phi_e(x)=W_{aug}^T \phi(x)\;(\phi_e(x)\in \mathbb{R}^{2k})</script><p>将$\phi_e(x)$分为两部分，分别为UA和LA，$\phi_e(x)=[\phi_{att}(x);\phi_{lat}(x)]\;(\phi_{att}(x),\phi_{att}(x) \in \mathbb{R}^{k})$</p>
<p>UA采用softmax loss:</p>
<script type="math/tex; mode=display">L_{att}=-\frac{1}{N}\sum_{i}^{n}log \frac{exp(\langle \phi_{att}(x),a \rangle)}{\sum_{c} exp(\langle \phi_{att}(x),a^{c} \rangle)},c \in Y_s</script><p>LA采用triplet loss:</p>
<script type="math/tex; mode=display">L_{lat}=max(0,m+d(\phi_{lat}(x_i),\phi_{lat}(x_k))-d(\phi_{lat}(x_i),\phi_{lat}(x_j)))</script><p>$x_i$和$x_k$是相同类别的图片，$x_j$是不同类别的图片</p>
<p>使类内距离最小，类间距离最大来保证学习到的属性是discriminative的，m为margin，设为1.0</p>
<p>由于有两个尺度(原图和ZNet后的图)，因此总的损失是由四个部分组成的</p>
<script type="math/tex; mode=display">L=L_{att}^{s1}+L_{lat}^{s1}+L_{att}^{s2}+L_{lat}^{s2}</script><h1 id="ZSL-Prediction"><a href="#ZSL-Prediction" class="headerlink" title="ZSL Prediction"></a>ZSL Prediction</h1><h2 id="Combining-multiple-spaces"><a href="#Combining-multiple-spaces" class="headerlink" title="Combining multiple spaces"></a>Combining multiple spaces</h2><p>用concated UA-LA feature来进行预测</p>
<script type="math/tex; mode=display">y^{*}=argmax_{c \in Y_u}(\langle [\phi_{att}(x);\phi_{lat}(x)],[a^c;\overline{\phi_{lat}^{c}}] \rangle)</script><script type="math/tex; mode=display">=argmax_{c \in Y_u}(\langle \phi_{att}(x),a^c \rangle + \langle \phi_{lat}(x),\overline{\phi_{lat}^{c}} \rangle)</script><h2 id="Combining-multiple-scales"><a href="#Combining-multiple-scales" class="headerlink" title="Combining multiple scales"></a>Combining multiple scales</h2><h3 id="multi-scale-UA-features"><a href="#multi-scale-UA-features" class="headerlink" title="multi-scale UA features"></a>multi-scale UA features</h3><p>先将两个尺度的特征catenate在一起:</p>
<script type="math/tex; mode=display">[\phi_{att}^{s1};\phi_{att}^{s2}]\in \mathbb{R}^{2k}</script><p>然后训练一个新的映射矩阵$W_{com}\in \mathbb{R}^{2k \times k}$</p>
<p>得到:</p>
<script type="math/tex; mode=display">\phi_{att}^{com}=W_{com}^T[\phi_{att}^{s1};\phi_{att}^{s2}]</script><h3 id="multi-scale-LA-features"><a href="#multi-scale-LA-features" class="headerlink" title="multi-scale LA features"></a>multi-scale LA features</h3><p>the combined feature can be obtained by directly concatenating the normalized two features</p>
<p>直接将两个向量归一化后连接在一起就可以得到组合特征</p>
<script type="math/tex; mode=display">\phi_{lat}^{com}=[\hat{\phi_{lat}^{s1}};\hat{\phi_{lat}^{s2}}]</script><p>至于为什么UA要映射到k维，我猜想是因为UA的维数固定，而LA的维数可以自己调整</p>
</div><div class="tags"><a href="/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">论文笔记</a></div><div class="post-nav"><a class="next" href="/2019/11/14/%E4%B8%80%E7%AF%87%E9%9B%B6%E6%A0%B7%E6%9C%AC%E5%AD%A6%E4%B9%A0%E7%9A%84%E7%BB%BC%E8%BF%B0%E2%80%94%E2%80%94%E9%92%88%E5%AF%B9%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E6%8B%86%E5%88%86/">一篇零样本学习的综述——针对数据集的拆分</a></div><div id="vcomment"></div><script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script><script src="//unpkg.com/valine@latest/dist/Valine.min.js"></script><script>var notify = 'false' ? true : false;
var verify = 'false' ? true : false;
var GUEST_INFO = ['nick','mail','link'];
var guest_info = 'nick,mail,link'.split(',').filter(function(item){
  return GUEST_INFO.indexOf(item) > -1
});
guest_info = guest_info.length == 0 ? GUEST_INFO :guest_info;
window.valine = new Valine({
  el:'#vcomment',
  notify:notify,
  verify:verify,
  appId:'OSUK7P1Utb45xOOhM0JJbTab-gzGzoHsz',
  appKey:'0OAn3KMQOEbFWSQB5P17tM7h',
  placeholder:'Just so so',
  avatar:'mm',
  guest_info:guest_info,
  pageSize:'10'
})</script></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank"><input type="text" name="q" maxlength="20" placeholder="Search"/><input type="hidden" name="sitesearch" value="http://hhhhhxh.github.io"/></form></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/%E6%95%99%E7%A8%8B/" style="font-size: 15px;">教程</a> <a href="/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/" style="font-size: 15px;">论文笔记</a> <a href="/tags/%E9%B8%A1%E6%B1%A4/" style="font-size: 15px;">鸡汤</a> <a href="/tags/%E8%B5%84%E6%96%99%E6%95%B4%E7%90%86/" style="font-size: 15px;">资料整理</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2019/11/27/%E5%AF%B9%E9%9B%B6%E6%A0%B7%E6%9C%AC%E5%AD%A6%E4%B9%A0%E4%B8%AD%E6%BD%9C%E5%9C%A8%E7%89%B9%E5%BE%81%E7%9A%84%E5%88%A4%E5%88%AB%E5%AD%A6%E4%B9%A0/">对零样本学习中潜在特征的判别学习</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/11/14/%E4%B8%80%E7%AF%87%E9%9B%B6%E6%A0%B7%E6%9C%AC%E5%AD%A6%E4%B9%A0%E7%9A%84%E7%BB%BC%E8%BF%B0%E2%80%94%E2%80%94%E9%92%88%E5%AF%B9%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E6%8B%86%E5%88%86/">一篇零样本学习的综述——针对数据集的拆分</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/11/12/SEVER%EF%BC%8C%E4%B8%80%E7%A7%8D%E6%A2%AF%E5%BA%A6%E4%BC%98%E5%8C%96%E7%9A%84%E9%B2%81%E6%A3%92%E5%85%83%E7%AE%97%E6%B3%95/">SEVER——一种梯度优化的鲁棒元算法</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/11/10/%E5%85%B1%E5%8B%89/">共勉</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/10/29/git-lfs%E7%9A%84%E4%BD%BF%E7%94%A8/">git lfs的使用</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/10/22/%E9%9B%B6%E6%A0%B7%E6%9C%AC%E5%AD%A6%E4%B9%A0ZSL%E8%B5%84%E6%96%99%E6%95%B4%E7%90%86/">零样本学习ZSL资料整理</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/10/18/%E8%A7%86%E8%A7%89%E9%97%AE%E7%AD%94VQA%E8%B5%84%E6%96%99%E6%95%B4%E7%90%86/">视觉问答VQA资料整理</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/10/07/hello-world/">Hello World</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="http://www.example1.com/" title="site-name1" target="_blank">site-name1</a><ul></ul><a href="http://www.example2.com/" title="site-name2" target="_blank">site-name2</a><ul></ul><a href="http://www.example3.com/" title="site-name3" target="_blank">site-name3</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2019 <a href="/." rel="nofollow">hhhhhxh.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>