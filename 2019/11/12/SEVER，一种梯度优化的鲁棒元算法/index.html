<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content=""><title>SEVER——一种梯度优化的鲁棒元算法 | hhhhhxh</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/8.0.1/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/3.4.0/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = 'https://hm.baidu.com/hm.js?' + '7fa17d1076a1ae6cf7ec6cc0ad18ad46';
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
  })();</script></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">SEVER——一种梯度优化的鲁棒元算法</h1><a id="logo" href="/.">hhhhhxh</a><p class="description"></p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a><a href="/photos/"><i class="fa fa-instagram"> 照片墙</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">SEVER——一种梯度优化的鲁棒元算法</h1><div class="post-meta">Nov 12, 2019<script src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> 阅读</span></span></div><a class="disqus-comment-count" href="/2019/11/12/SEVER%EF%BC%8C%E4%B8%80%E7%A7%8D%E6%A2%AF%E5%BA%A6%E4%BC%98%E5%8C%96%E7%9A%84%E9%B2%81%E6%A3%92%E5%85%83%E7%AE%97%E6%B3%95/#vcomment"><span class="valine-comment-count" data-xid="/2019/11/12/SEVER%EF%BC%8C%E4%B8%80%E7%A7%8D%E6%A2%AF%E5%BA%A6%E4%BC%98%E5%8C%96%E7%9A%84%E9%B2%81%E6%A3%92%E5%85%83%E7%AE%97%E6%B3%95/"></span><span> 条评论</span></a><div class="clear"><div class="toc-article" id="toc"><div class="toc-title">文章目录</div><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Introduction"><span class="toc-number">1.</span> <span class="toc-text">Introduction</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Formal-Setting"><span class="toc-number">2.</span> <span class="toc-text">Formal Setting</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Definition-2-1-varepsilon-扰动模型-varepsilon-contamination-model"><span class="toc-number">2.1.</span> <span class="toc-text">Definition 2.1 $\varepsilon$-扰动模型($\varepsilon$-contamination model)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Definition-2-2-gamma-近似临界点-gamma-approximate-critical-point"><span class="toc-number">2.2.</span> <span class="toc-text">Definition 2.2 $\gamma$-近似临界点($\gamma$-approximate critical point)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Definition-2-3-gamma-近似学习器-gamma-approximate-learner"><span class="toc-number">2.3.</span> <span class="toc-text">Definition 2.3 $\gamma$-近似学习器($\gamma$-approximate learner)</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Algorithm"><span class="toc-number">3.</span> <span class="toc-text">Algorithm</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Theoretical-Guarantees"><span class="toc-number">4.</span> <span class="toc-text">Theoretical Guarantees</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Theorem-2-1"><span class="toc-number">4.1.</span> <span class="toc-text">Theorem 2.1</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Proposition-2-2-非正式的"><span class="toc-number">4.2.</span> <span class="toc-text">Proposition 2.2 (非正式的)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Corollary-2-3"><span class="toc-number">4.3.</span> <span class="toc-text">Corollary 2.3</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Practical-Considerations"><span class="toc-number">5.</span> <span class="toc-text">Practical Considerations</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Overview-of-Sever-and-its-Analysis"><span class="toc-number">6.</span> <span class="toc-text">Overview of Sever and its Analysis</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Experiments"><span class="toc-number">7.</span> <span class="toc-text">Experiments</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Ridge-Regression"><span class="toc-number">7.1.</span> <span class="toc-text">Ridge Regression</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Support-Vector-Machine"><span class="toc-number">7.2.</span> <span class="toc-text">Support Vector Machine</span></a></li></ol></li></ol></div></div><div class="post-content"><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>这篇论文的来源是2019年的ICML，主要提出了一种能够吸收基础学习器(如最小二乘，SGD)的元算法，能够加固学习器使得它不受异常值的影响。此外，这个方法也可以拓展到高维空间上，因为除了需要运行基础学习器，它只需要计算一个特定的$n*d$矩阵的顶部奇异向量。</p>
<p>论文首先介绍了异常数据在生物数据和机器学习安全上的影响，因此工作聚焦于设计一个对于异常值鲁棒的算法，同时又要保持较好的准确率和运行时间。而存在异常时的估计是鲁棒统计中的一个典型目标，但现在流行的方法都不能很好的解决这个问题。当前有效的鲁棒估计未能实现实际的高维鲁棒估计，并且他们存在下面两个缺点中的一个：</p>
<ol>
<li>用了复杂的凸优化算法，使得它并不适用于大型数据集</li>
<li>仅适用于特定问题或特定的数据分布，在实际数据上的准确率不行</li>
</ol>
<p>整体来看，我们的算法是一个简单的异常值检测器插件——首先，运行可以正常运行的任何学习过程(如最小平方，线性回归)。然后，考虑最佳参数处的梯度矩阵，并计算该矩阵的顶部奇异向量。最后，删除所有在这个奇异向量上的投影太大的点(并且如果必要的话可以重新训练)，如下图所示。</p>
<p><img src="/images/《SEVER-A-Robust-Meta-Algorithm-for-Stochastic-Optimization》论文笔记/figure1.png" alt=""></p>
<h1 id="Formal-Setting"><a href="#Formal-Setting" class="headerlink" title="Formal Setting"></a>Formal Setting</h1><p>首先考虑随机优化任务：对于函数$f$ : $H \to \mathbb{R}$，其真实分布为$p^*$</p>
<p>目标是找到一个参数向量$w^* \in H$使得这个式子最小</p>
<script type="math/tex; mode=display">\overline{f(w)} \overset{def}{=}\mathbb{E}_{f \sim p^{*}}[f(w)]</script><p>假设$H \subseteq \mathbb{R}^d$，为可能参数的空间</p>
<p>为了学习参数向量$w^*$，我们可以用有n个函数的训练集：</p>
<script type="math/tex; mode=display">f_{1:n} \overset{def}{=} \left\{ f_1,...,f_n \right\}</script><p>如对线性回归来说，$f_i (w)=\frac{1}{2}(wx_i-y_i)^2$，其中$(x_i,y_i)$是观察到的数据点</p>
<p>然而，我们不像没有异常值的那样去假设$f_1,…,f_n \sim p^*$，我们允许$\varepsilon$-部分的点可以有任意的异常值</p>
<p>接下来给出一些定义(编号按照论文)：</p>
<h2 id="Definition-2-1-varepsilon-扰动模型-varepsilon-contamination-model"><a href="#Definition-2-1-varepsilon-扰动模型-varepsilon-contamination-model" class="headerlink" title="Definition 2.1 $\varepsilon$-扰动模型($\varepsilon$-contamination model)"></a>Definition 2.1 $\varepsilon$-扰动模型($\varepsilon$-contamination model)</h2><p>给定$\varepsilon&gt;0$，以及函数$f$ : $H \to \mathbb{R}$的分布$p^*$</p>
<p>数据按如下规则生成：</p>
<ol>
<li>从分布$p^*$中得到$n$个无异常值的样本$f_1,…,f_n$</li>
<li>从这些样本中选择任意$\varepsilon n$个，替换为任意样本</li>
</ol>
<p>得到的数据点集合被传给算法，我们称这样得到的样本集合为$\varepsilon$-扰动的(关于$p^*$)</p>
<p>在$\varepsilon$-扰动模型中，允许添加及删除数据点。我们的理论结果保持在这个非常鲁棒的模型中。我们的实验评估使用了含有异常值的实例，其仅允许添加异常点。本质上，加性扰动与鲁棒统计中Huber的扰动模型相符。</p>
<p>最后，我们通常会假设用一个黑盒学习器，用$L$表示，输入函数$f_1,…,f_n$，输出一个参数向量$w \in H$。我们想要约定$L$近似最小化了$\frac{1}{n} \sum^{n}_{i=1}f_i (w)$。为了达到这个目的，介绍下面这个定义。</p>
<h2 id="Definition-2-2-gamma-近似临界点-gamma-approximate-critical-point"><a href="#Definition-2-2-gamma-近似临界点-gamma-approximate-critical-point" class="headerlink" title="Definition 2.2 $\gamma$-近似临界点($\gamma$-approximate critical point)"></a>Definition 2.2 $\gamma$-近似临界点($\gamma$-approximate critical point)</h2><p>给定函数$f$ : $H \to \mathbb{R}$，$f$的一个$\gamma$-近似临界点是一个点$w \in H$，对于所有的单位向量$v$和任意小的正数$\delta$，都有</p>
<script type="math/tex; mode=display">w+\delta v \in H</script><p>同时，我们有</p>
<script type="math/tex; mode=display">v \cdot \bigtriangledown f(w) \geq - \gamma</script><p>本质上来讲，上面的定义意味着$f$的值不能通过停留在域内的同时改变局部输入$w$来减少很多。这个条件迫使在任意的方向$v$上移动，要么导致我们离开$H$，要么导致$f$以不超过$\gamma$的速率下降。需要注意的是当$H=\mathbb{R}^d$时，我们上面的近似临界点的概念就退化为标准的近似不动点(一个梯度大小非常小的点)的概念。</p>
<p>接下来，我们可以提出$\gamma$-近似学习器的概念。</p>
<h2 id="Definition-2-3-gamma-近似学习器-gamma-approximate-learner"><a href="#Definition-2-3-gamma-近似学习器-gamma-approximate-learner" class="headerlink" title="Definition 2.3 $\gamma$-近似学习器($\gamma$-approximate learner)"></a>Definition 2.3 $\gamma$-近似学习器($\gamma$-approximate learner)</h2><p>一个学习算法$L$被认为是$\gamma$-近似的当对于任意的函数</p>
<script type="math/tex; mode=display">f_1,...,f_n：H \to \mathbb{R}</script><p>每一个都被界定在封闭域$H$下，$L$的输出$w=L(f_{1:n})$是</p>
<script type="math/tex; mode=display">f(x):=\frac{1}{n} \sum_{i=1}^{n}f_i (x)</script><p>的$\gamma$-近似临界点</p>
<p>换句话说，$L$总是能找到经验学习目标的近似临界点。我们注意到大多数常见的学习算法(如SGD)满足$\gamma$-近似学习器的性质。对于我们的线性回归例子来说，可以用梯度$\frac{1}{n} \sum_{i=1}^{n} x_i(w \cdot x_i-y_i)$来执行梯度下降。然而，在某些情况下，一个更加有效率并且直接的方式是把梯度设置为0并求解$w$。在我们的线性回归例子中，这样可以得到最优参数向量的闭式解。</p>
<h1 id="Algorithm"><a href="#Algorithm" class="headerlink" title="Algorithm"></a>Algorithm</h1><p><img src="/images/《SEVER-A-Robust-Meta-Algorithm-for-Stochastic-Optimization》论文笔记/figure1.png" alt=""></p>
<p>如图中所示，我们的算法通过对黑盒学习算法的梯度进行后处理来产生效果。基本的直觉是这样的：我们要确保异常值不会对学习的参数产生较大影响。 从直觉上讲，为了使异常值具有这种效果，其对应的梯度应</p>
<ol>
<li>比较大</li>
<li>系统地指向某一特定方向</li>
</ol>
<p>我们可以通过奇异值分解来检测——如果1和2都成立，那么异常值应该是梯度矩阵中较大的奇异值的原因，这使我们能够检测到并去除它们。</p>
<p>算法1中的伪代码更好的展示了这点。</p>
<p><img src="/images/《SEVER-A-Robust-Meta-Algorithm-for-Stochastic-Optimization》论文笔记/algorithm1.png" alt=""></p>
<p><img src="/images/《SEVER-A-Robust-Meta-Algorithm-for-Stochastic-Optimization》论文笔记/algorithm2.png" alt=""></p>
<p>具体来说，我们描述该算法是如何在线性回归中发挥作用的。首先，我们会解出数据集上的最佳参数向量，不考虑鲁棒性的问题。</p>
<p>具体来说，令$\hat{w}$为</p>
<script type="math/tex; mode=display">\sum_{i=1}^{n} x_i(\hat{w} \cdot x_i - y_i)=0</script><p>的解，令梯度为0，根据我们的需要给出一个临界点。</p>
<p>我们计算出平均梯度$\frac{1}{n}\sum_{i=1}^{n} x_i(\hat{w} \cdot x_i - y_i)$，并用这个来计算中心梯度矩阵$G$。也就是说，$G$的第$j$行，$G_j$，为向量</p>
<script type="math/tex; mode=display">x_j(\hat{w} \cdot x_j-y_j)-\frac{1}{n}\sum^{n}_{i=1}x_i(\hat{w} \cdot x_i-y_i)</script><p>我们计算出$G$的右上奇异向量，把数据投影到这个方向，然后将所得到的值平方来得出每个点的分数</p>
<script type="math/tex; mode=display">\tau _j = (G_j \cdot v)^2</script><p>有了这些分数后，我们运行算法2，来(随机的)去除那些分数最大的点。我们在这个点的子集上重新运行整个过程，直到算法2不再去除任何一个点。</p>
<h1 id="Theoretical-Guarantees"><a href="#Theoretical-Guarantees" class="headerlink" title="Theoretical Guarantees"></a>Theoretical Guarantees</h1><p>我们的第一个理论结果表明，只要数据不是太重尾(<a href="https://baike.baidu.com/item/重尾分，布/4483429" target="_blank" rel="noopener">重尾分布</a>)，即使存在异常值，SEVER都会找到真实函数$\overline{f}$的近似临界点。</p>
<h2 id="Theorem-2-1"><a href="#Theorem-2-1" class="headerlink" title="Theorem 2.1"></a>Theorem 2.1</h2><p>假设函数$f_1,…,f_n,\overline{f}: H \to \mathbb{R}$被界定在封闭域$H$下，并假设他们都满足如下的确定性规律性条件：</p>
<p>存在集合$I_{good} \subseteq [n]$，其中$|I_{good}| \geq (1 - \varepsilon)n$并且$\sigma&gt;0$</p>
<p>因此：</p>
<ol>
<li><p>$Cov_{I_{good}}[\bigtriangledown f_i(w)] \preceq \sigma^{2}I, w \in H$</p>
</li>
<li><p>$||\bigtriangledown \hat{f}(w) - \bigtriangledown \overline{f}(w)||_{2} \leq \sigma \sqrt{\varepsilon}, w \in H$</p>
<p>其中$\hat{f}\overset{def}{=} \frac{1}{|I_{good}|}\sum_{i \in I_{good}}f_{i}$</p>
</li>
</ol>
<p>这样，我们的算法应用于$f_1,…,f_n,\sigma$返回的是一个点$w \in H$，至少$\frac{9}{10}$的可能性是$\overline{f}$的一个$(\gamma+O(\sigma \sqrt{\varepsilon}))$-近似临界点。</p>
<p>定理2.1的关键点在于，对错误的保证并不依赖于基础维度$d$。相反，大多数自然算法都会随着$d$的增加而增加，因此在高维中的鲁棒性很差。</p>
<p>我们证明了在一些关于$p^{*}$的优美假设下，多项式的样本非常有可能满足确定性规律性条件：</p>
<h2 id="Proposition-2-2-非正式的"><a href="#Proposition-2-2-非正式的" class="headerlink" title="Proposition 2.2 (非正式的)"></a>Proposition 2.2 (非正式的)</h2><p>令$H \in \mathbb{R}^{d}$为一个最大直径为$r$的封闭边界集</p>
<p>$p^{*}$为函数$f: H \to \mathbb{R}$的分布</p>
<p>$\overline{f}=\mathbb{E}_{f \sim p^{*}}[f]$</p>
<p>假设对于任意的$w \in H$和单位向量$v$我们都有</p>
<script type="math/tex; mode=display">\mathbb{E}_{f \sim p^{*}}[(v \cdot (\bigtriangledown{f(w)}-\overline{f}(w)))^{2}] \leq \sigma^2</script><p>在适当的Lipschitz和平滑假设下，对于$n=\Omega(dlog(r/(\sigma^2 \varepsilon))/(\sigma^2 \varepsilon))$，一个从分布$p^{*}$中得到的独立同分布的$\varepsilon$-扰动的函数集合有很大的概率满足条件1和条件2。</p>
<p>由于定理2.1非常笼统，甚至可以用于非凸的损失函数，但我们通常希望不只是一个近似临界点。特别是对于凸问题，我们可以保证能找到一个近似的全局最小值。这是定理2.1的推论：</p>
<h2 id="Corollary-2-3"><a href="#Corollary-2-3" class="headerlink" title="Corollary 2.3"></a>Corollary 2.3</h2><p>假设$f_1,…,f_n: H \to \mathbb{R}$满足规律性条件1和条件2，并且$H$是凸的，具有$l_2$半径$r$。这样，SEVER的输出至少有$\frac{9}{10}$的概率满足如下式子：</p>
<ol>
<li><p>若$\overline{f}$是凸的，算法能够找到一个点$w \in H$满足$\overline{f}(w)-\overline{f}(w^{*})=O((\sigma \sqrt{\varepsilon}+\gamma)r)$ </p>
</li>
<li><p>若$\overline{f}$是$\xi$强凸($\xi$-strongly convex)的，算法能够找到一个点$w \in H$满足$\overline{f}(w)-\overline{f}(w^{*})=O((\varepsilon \sigma^{2}+\gamma^{2})/\xi)$ </p>
</li>
</ol>
<blockquote>
<p>$f$ is $ \xi $-strongly convex if $f(x)-\frac{\xi}{2}x^{T}x$ is convex.</p>
<p>一个函数减去二次函数仍然是凸的，说明它至少有这个二次函数这么凸。或者说这个二次函数是它的一个下界。</p>
<p>参考<a href="https://blog.fangzhou.me/posts/20190217-convex-function-lipschitz-smooth-strongly-convex/" target="_blank" rel="noopener">凸函数，Lipschitz smooth, strongly convex</a></p>
</blockquote>
<h1 id="Practical-Considerations"><a href="#Practical-Considerations" class="headerlink" title="Practical Considerations"></a>Practical Considerations</h1><p>从理论上来说，我们应该是每次跑算法2，直到算法1的第11行中的停止条件被满足。但在实际实验中并没有这么做，而是在每次迭代中，简单地根据分数$\tau_i$去掉最上面的$p$部分的异常值，并且跑$r$轮，而不是根据停止条件来停止迭代。实验部分就是这么做的。</p>
<h1 id="Overview-of-Sever-and-its-Analysis"><a href="#Overview-of-Sever-and-its-Analysis" class="headerlink" title="Overview of Sever and its Analysis"></a>Overview of Sever and its Analysis</h1><p>为了简化问题，我们限制问题中遇到的函数是凸的。</p>
<p>对于凸函数，其概率分布为$p^*$</p>
<p>定义在一些凸域$H \in \mathbb{R}^d$上</p>
<p>我们想要最小化函数</p>
<script type="math/tex; mode=display">\overline{f}=\mathbb{E}_{f \sim p^{*}}[f]</script><p>在不存在异常值的情况下，这个问题很好理解，如果我们以分布$p^{*}$拿出足够多的样本，它们的均值$\hat{f}$有极大概率近似于$\overline{f}$，因此，我们可以用凸优化中的标准方法来寻找$\hat{f}$的近似最小化，反过来将其用作$\overline{f}$的近似最小化。</p>
<p>在鲁棒情况下，随机优化变得很有挑战性：即使对于最基本的特殊条件(如均值估计，线性回归)，仅仅一个异常的样本都可以大大改变$\hat{f}$的最小值的位置。此外，简单的异常值去除方法只能应对很小一部分的异常值(对应于$\varepsilon=O(d^{-1/2})$)。</p>
<p>为了解决这个问题，我们的第一个想法是：考虑标准(投影)梯度下降方法来求出$\hat{f}$的最小值。该算法会在适当的点重复计算$\hat{f}$的梯度并使用它来更新当前位置，并重复这个过程。问题在于，异常值会完全改变该算法的行为，因为它们可以大幅改变在选定点上$\hat{f}$的梯度。关键的观察结果在于，在可以访问$\varepsilon$-扰动的数据集的情况下，在给定点上近似$\overline{f}$的梯度可以被认为是鲁棒均值估计问题。因此，我们可以用[<a href="https://arxiv.org/abs/1703.00893" target="_blank" rel="noopener">DKK$^{+}$17</a>]中的鲁棒均值估计算法，该算法在较好的样本并且假设相当温和的情况下可以成功。假设满足$f \sim p^{*}$的$\bigtriangledown f(x)$的协方差矩阵是有界的，我们就可以“模拟”梯度下降，计算$\overline{f}$的近似最小值。</p>
<p>总之，第一种算法思想就是用鲁棒的均值估计程序作为黑盒，以便在(投影)梯度下降的每次迭代中鲁棒地估计梯度。这样就产生了一个简单鲁棒的随机优化方法，该方法在非常普适的情况下都具有多项式样本的复杂性和运行时间。</p>
<p>现在我们可以描述SEVER(算法1)以及其背后的主要见解了。粗略地说，只有当算法达到$\hat{f}$的近似临界点时，SEVER才会调用我们的鲁棒均值估计程序(实际上是[DKK$^{+}17$]的用于去除异常值的过滤方法)。这个方法有两个主要动机：</p>
<p>第一，根据经验，我们观察到，如果我们迭代过滤样本，保持样本被删除的子集，那么过滤器只有很少次数的迭代会删除点。</p>
<p>第二，滤波器子线程(算法2)的迭代比梯度下降的迭代开销更大。</p>
<p>因此，在连续的滤波之间对当前存在异常样本的样本集运行许多步梯度下降是有利的。这个想法可以通过用随机梯度下降而不是在每个步骤计算平均值来进一步改善。</p>
<p>我们分析的一个重要特征是SEVER并未将鲁棒的均值估计程序作为一个黑盒来使用。相反，我们利用了我们的过滤算法的性能保证。分析的主要思想是这样的：</p>
<p>假设我们达到了$\hat{f}$的一个近似临界点$w$，在这一步我们应用我们的过滤算法。通过后一个算法的性能保证，我们会处于下面两种情况之一：</p>
<ol>
<li>过滤算法删除了一组损坏的函数</li>
<li>它证明了$\hat{f}$的梯度更接近$\overline{f}$在$w$的梯度。</li>
</ol>
<p>对于第一种情况，我们在生产更“清洁”的数据集上取得了进展。</p>
<p>对于第二种情况，我们的证明意味着$w$也是$\overline{f}$的近似临界点，那就搞定了。</p>
<h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><p><a href="https: //github.com/hoonose/sever" target="_blank" rel="noopener">github</a></p>
<p>作为基础学习器，我们分别用了脊回归和SVM。</p>
<p>在这两种情况下，我们都运行基础学习器，然后在学习到的参数处为每个数据点提取梯度。然后，我们将梯度居中，并运行MATLAB的svds方法来计算顶部奇异矢量$v$，并删除具有最大的异常值得分$\tau_{i}$的点$i$的顶部$p$部分，这个分数由对$v$的投影的大小的平方计算得出(见算法1)。我们总共重复了$r$次迭代。对于分类，我们将每个类别的梯度分别居中(并分别去除点)，从而提高了性能。</p>
<p>我们将我们的方法与六个基准方法进行了比较。除了其中一个以外，所有其他人都具有与Sever相同的高级形式(运行基础学习器，然后过滤得分最高的点的前$p$部分)，但是使用分数$\tau_{i}$的不同定义来确定要过滤的点。</p>
<p>实验结果：</p>
<h2 id="Ridge-Regression"><a href="#Ridge-Regression" class="headerlink" title="Ridge Regression"></a>Ridge Regression</h2><p><img src="/images/《SEVER-A-Robust-Meta-Algorithm-for-Stochastic-Optimization》论文笔记/figure2.png" alt=""></p>
<p><img src="/images/《SEVER-A-Robust-Meta-Algorithm-for-Stochastic-Optimization》论文笔记/figure3.png" alt=""></p>
<h2 id="Support-Vector-Machine"><a href="#Support-Vector-Machine" class="headerlink" title="Support Vector Machine"></a>Support Vector Machine</h2><p><img src="/images/《SEVER-A-Robust-Meta-Algorithm-for-Stochastic-Optimization》论文笔记/figure4.png" alt=""></p>
<p><img src="/images/《SEVER-A-Robust-Meta-Algorithm-for-Stochastic-Optimization》论文笔记/figure5.png" alt=""></p>
<p><img src="/images/《SEVER-A-Robust-Meta-Algorithm-for-Stochastic-Optimization》论文笔记/figure6.png" alt=""></p>
</div><div class="tags"><a href="/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">论文笔记</a></div><div class="post-nav"><a class="pre" href="/2019/11/14/%E4%B8%80%E7%AF%87%E9%9B%B6%E6%A0%B7%E6%9C%AC%E5%AD%A6%E4%B9%A0%E7%9A%84%E7%BB%BC%E8%BF%B0%E2%80%94%E2%80%94%E9%92%88%E5%AF%B9%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E6%8B%86%E5%88%86/">一篇零样本学习的综述——针对数据集的拆分</a><a class="next" href="/2019/11/10/%E5%85%B1%E5%8B%89/">共勉</a></div><div id="vcomment"></div><script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script><script src="//unpkg.com/valine@latest/dist/Valine.min.js"></script><script>var notify = 'false' ? true : false;
var verify = 'false' ? true : false;
var GUEST_INFO = ['nick','mail','link'];
var guest_info = 'nick,mail,link'.split(',').filter(function(item){
  return GUEST_INFO.indexOf(item) > -1
});
guest_info = guest_info.length == 0 ? GUEST_INFO :guest_info;
window.valine = new Valine({
  el:'#vcomment',
  notify:notify,
  verify:verify,
  appId:'OSUK7P1Utb45xOOhM0JJbTab-gzGzoHsz',
  appKey:'0OAn3KMQOEbFWSQB5P17tM7h',
  placeholder:'Just so so',
  avatar:'mm',
  guest_info:guest_info,
  pageSize:'10'
})</script></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank"><input type="text" name="q" maxlength="20" placeholder="Search"/><input type="hidden" name="sitesearch" value="http://hhhhhxh.github.io"/></form></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/tutorial/" style="font-size: 15px;">tutorial</a> <a href="/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/" style="font-size: 15px;">论文笔记</a> <a href="/tags/%E8%B5%84%E6%96%99%E6%95%B4%E7%90%86/" style="font-size: 15px;">资料整理</a> <a href="/tags/%E9%B8%A1%E6%B1%A4/" style="font-size: 15px;">鸡汤</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2020/01/01/2020-Flag/">2020 Flag</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/12/13/AdaGrad-RMSProp-Adam%E7%AE%97%E6%B3%95%E7%9A%84%E7%AE%80%E5%8D%95%E7%90%86%E8%A7%A3/">AdaGrad,RMSProp,Adam算法的简单理解</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/11/27/%E5%AF%B9%E9%9B%B6%E6%A0%B7%E6%9C%AC%E5%AD%A6%E4%B9%A0%E4%B8%AD%E6%BD%9C%E5%9C%A8%E7%89%B9%E5%BE%81%E7%9A%84%E5%88%A4%E5%88%AB%E5%AD%A6%E4%B9%A0/">对零样本学习中潜在特征的判别学习</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/11/14/%E4%B8%80%E7%AF%87%E9%9B%B6%E6%A0%B7%E6%9C%AC%E5%AD%A6%E4%B9%A0%E7%9A%84%E7%BB%BC%E8%BF%B0%E2%80%94%E2%80%94%E9%92%88%E5%AF%B9%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E6%8B%86%E5%88%86/">一篇零样本学习的综述——针对数据集的拆分</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/11/12/SEVER%EF%BC%8C%E4%B8%80%E7%A7%8D%E6%A2%AF%E5%BA%A6%E4%BC%98%E5%8C%96%E7%9A%84%E9%B2%81%E6%A3%92%E5%85%83%E7%AE%97%E6%B3%95/">SEVER——一种梯度优化的鲁棒元算法</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/11/10/%E5%85%B1%E5%8B%89/">共勉</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/10/29/git-lfs%E7%9A%84%E4%BD%BF%E7%94%A8/">git lfs的使用</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/10/22/%E9%9B%B6%E6%A0%B7%E6%9C%AC%E5%AD%A6%E4%B9%A0ZSL%E8%B5%84%E6%96%99%E6%95%B4%E7%90%86/">零样本学习ZSL资料整理</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/10/18/%E8%A7%86%E8%A7%89%E9%97%AE%E7%AD%94VQA%E8%B5%84%E6%96%99%E6%95%B4%E7%90%86/">视觉问答VQA资料整理</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/10/07/hello-world/">Hello World</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="http://www.example1.com/" title="site-name1" target="_blank">site-name1</a><ul></ul><a href="http://www.example2.com/" title="site-name2" target="_blank">site-name2</a><ul></ul><a href="http://www.example3.com/" title="site-name3" target="_blank">site-name3</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2020 <a href="/." rel="nofollow">hhhhhxh.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>