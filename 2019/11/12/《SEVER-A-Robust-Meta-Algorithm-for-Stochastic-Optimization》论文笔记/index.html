<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content=""><title>《SEVER: A Robust Meta-Algorithm for Stochastic Optimization》论文笔记 | hhhhhxh</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/8.0.1/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/3.4.0/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = 'https://hm.baidu.com/hm.js?' + '7fa17d1076a1ae6cf7ec6cc0ad18ad46';
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
  })();</script></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">《SEVER: A Robust Meta-Algorithm for Stochastic Optimization》论文笔记</h1><a id="logo" href="/.">hhhhhxh</a><p class="description"></p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a><a href="/photos/"><i class="fa fa-instagram"> 照片墙</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">《SEVER: A Robust Meta-Algorithm for Stochastic Optimization》论文笔记</h1><div class="post-meta">Nov 12, 2019<script src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> 阅读</span></span></div><a class="disqus-comment-count" href="/2019/11/12/%E3%80%8ASEVER-A-Robust-Meta-Algorithm-for-Stochastic-Optimization%E3%80%8B%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/#vcomment"><span class="valine-comment-count" data-xid="/2019/11/12/%E3%80%8ASEVER-A-Robust-Meta-Algorithm-for-Stochastic-Optimization%E3%80%8B%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"></span><span> 条评论</span></a><div class="clear"><div class="toc-article" id="toc"><div class="toc-title">文章目录</div><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#简介"><span class="toc-number">1.</span> <span class="toc-text">简介</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Formal-Setting"><span class="toc-number">2.</span> <span class="toc-text">Formal Setting</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#定义2-1-varepsilon-扰动模型-varepsilon-contamination-model"><span class="toc-number">2.1.</span> <span class="toc-text">定义2.1 $\varepsilon$-扰动模型($\varepsilon$-contamination model)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#定义2-2-gamma-近似临界点-gamma-approximate-critical-point"><span class="toc-number">2.2.</span> <span class="toc-text">定义2.2 $\gamma$-近似临界点($\gamma$-approximate critical point)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#定义2-3-gamma-近似学习器-gamma-approximate-learner"><span class="toc-number">2.3.</span> <span class="toc-text">定义2.3 $\gamma$-近似学习器($\gamma$-approximate learner)</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Algorithm"><span class="toc-number">3.</span> <span class="toc-text">Algorithm</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Theoretical-Guarantees"><span class="toc-number">4.</span> <span class="toc-text">Theoretical Guarantees</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#定理2-1"><span class="toc-number">4.1.</span> <span class="toc-text">定理2.1</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#论点2-2-非正式的"><span class="toc-number">4.2.</span> <span class="toc-text">论点2.2 (非正式的)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#推论2-3"><span class="toc-number">4.3.</span> <span class="toc-text">推论2.3</span></a></li></ol></li></ol></div></div><div class="post-content"><h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>这篇论文的来源是2019年的ICML，主要提出了一种能够吸收基础学习器(如最小二乘，SGD)的元算法，能够加固学习器使得它不受异常值的影响。此外，这个方法也可以拓展到高维空间上，因为除了需要运行基础学习器，它只需要计算一个特定的$n*d$矩阵的顶部奇异向量。</p>
<p>论文首先介绍了异常数据在生物数据和机器学习安全上的影响，因此工作聚焦于设计一个对于异常值鲁棒的算法，同时又要保持较好的准确率和运行时间。而存在异常时的估计是鲁棒统计中的一个典型目标，但现在流行的方法都不能很好的解决这个问题。当前有效的鲁棒估计未能实现实际的高维鲁棒估计，并且他们存在下面两个缺点中的一个：</p>
<ol>
<li>用了复杂的凸优化算法，使得它并不适用于大型数据集</li>
<li>仅适用于特定问题或特定的数据分布，在实际数据上的准确率不行</li>
</ol>
<p>整体来看，我们的算法是一个简单的异常值检测器插件——首先，运行可以正常运行的任何学习过程(如最小平方，线性回归)。然后，考虑最佳参数处的梯度矩阵，并计算该矩阵的顶部奇异向量。最后，删除所有在这个奇异向量上的投影太大的点(并且如果必要的话可以重新训练)，如下图所示。</p>
<p><img src="/images/《SEVER-A-Robust-Meta-Algorithm-for-Stochastic-Optimization》论文笔记/1.png" alt=""></p>
<h1 id="Formal-Setting"><a href="#Formal-Setting" class="headerlink" title="Formal Setting"></a>Formal Setting</h1><p>首先考虑随机优化任务：对于函数$f$ : $H \to \mathbb{R}$，其真实分布为$p^*$</p>
<p>目标是找到一个参数向量$w^* \in H$使得这个式子最小</p>
<script type="math/tex; mode=display">\overline{f(w)} \overset{def}{=}\mathbb{E}_{f \sim p^{*}}[f(w)]</script><p>假设$H \subseteq \mathbb{R}^d$，为可能参数的空间</p>
<p>为了学习参数向量$w^*$，我们可以用有n个函数的训练集：</p>
<script type="math/tex; mode=display">f_{1:n} \overset{def}{=} \left\{ f_1,...,f_n \right\}</script><p>如对线性回归来说，$f_i (w)=\frac{1}{2}(wx_i-y_i)^2$，其中$(x_i,y_i)$是观察到的数据点</p>
<p>然而，我们不像没有异常值的那样去假设$f_1,…,f_n \sim p^*$，我们允许$\varepsilon$-部分的点可以有任意的异常值</p>
<p>接下来给出一些定义(编号按照论文)：</p>
<h2 id="定义2-1-varepsilon-扰动模型-varepsilon-contamination-model"><a href="#定义2-1-varepsilon-扰动模型-varepsilon-contamination-model" class="headerlink" title="定义2.1 $\varepsilon$-扰动模型($\varepsilon$-contamination model)"></a>定义2.1 $\varepsilon$-扰动模型($\varepsilon$-contamination model)</h2><p>给定$\varepsilon&gt;0$，以及函数$f$ : $H \to \mathbb{R}$的分布$p^*$</p>
<p>数据按如下规则生成：</p>
<ol>
<li>从分布$p^*$中得到$n$个无异常值的样本$f_1,…,f_n$</li>
<li>从这些样本中选择任意$\varepsilon n$个，替换为任意样本</li>
</ol>
<p>得到的数据点集合被传给算法，我们称这样得到的样本集合为$\varepsilon$-扰动的(关于$p^*$)</p>
<p>在$\varepsilon$-扰动模型中，允许添加及删除数据点。我们的理论结果保持在这个非常鲁棒的模型中。我们的实验评估使用了含有异常值的实例，其仅允许添加异常点。本质上，加性扰动与鲁棒统计中Huber的扰动模型相符。</p>
<p>最后，我们通常会假设用一个黑盒学习器，用$L$表示，输入函数$f_1,…,f_n$，输出一个参数向量$w \in H$。我们想要约定$L$近似最小化了$\frac{1}{n} \sum^{n}_{i=1}f_i (w)$。为了达到这个目的，介绍下面这个定义。</p>
<h2 id="定义2-2-gamma-近似临界点-gamma-approximate-critical-point"><a href="#定义2-2-gamma-近似临界点-gamma-approximate-critical-point" class="headerlink" title="定义2.2 $\gamma$-近似临界点($\gamma$-approximate critical point)"></a>定义2.2 $\gamma$-近似临界点($\gamma$-approximate critical point)</h2><p>给定函数$f$ : $H \to \mathbb{R}$，$f$的一个$\gamma$-近似临界点是一个点$w \in H$，对于所有的单位向量$v$和任意小的正数$\delta$，都有</p>
<script type="math/tex; mode=display">w+\delta v \in H</script><p>同时，我们有</p>
<script type="math/tex; mode=display">v \cdot \bigtriangledown f(w) \geq - \gamma</script><p>本质上来讲，上面的定义意味着$f$的值不能通过停留在域内的同时改变局部输入$w$来减少很多。这个条件迫使在任意的方向$v$上移动，要么导致我们离开$H$，要么导致$f$以不超过$\gamma$的速率下降。需要注意的是当$H=\mathbb{R}^d$时，我们上面的近似临界点的概念就退化为标准的近似不动点(一个梯度大小非常小的点)的概念。</p>
<p>接下来，我们可以提出$\gamma$-近似学习器的概念。</p>
<h2 id="定义2-3-gamma-近似学习器-gamma-approximate-learner"><a href="#定义2-3-gamma-近似学习器-gamma-approximate-learner" class="headerlink" title="定义2.3 $\gamma$-近似学习器($\gamma$-approximate learner)"></a>定义2.3 $\gamma$-近似学习器($\gamma$-approximate learner)</h2><p>一个学习算法$L$被认为是$\gamma$-近似的当对于任意的函数</p>
<script type="math/tex; mode=display">f_1,...,f_n：H \to \mathbb{R}</script><p>每一个都被界定在封闭域$H$下，$L$的输出$w=L(f_{1:n})$是</p>
<script type="math/tex; mode=display">f(x):=\frac{1}{n} \sum_{i=1}^{n}f_i (x)</script><p>的$\gamma$-近似临界点</p>
<p>换句话说，$L$总是能找到经验学习目标的近似临界点。我们注意到大多数常见的学习算法(如SGD)满足$\gamma$-近似学习器的性质。对于我们的线性回归例子来说，可以用梯度$\frac{1}{n} \sum_{i=1}^{n} x_i(w \cdot x_i-y_i)$来执行梯度下降。然而，在某些情况下，一个更加有效率并且直接的方式是把梯度设置为0并求解$w$。在我们的线性回归例子中，这样可以得到最优参数向量的闭式解。</p>
<h1 id="Algorithm"><a href="#Algorithm" class="headerlink" title="Algorithm"></a>Algorithm</h1><p><img src="/images/《SEVER-A-Robust-Meta-Algorithm-for-Stochastic-Optimization》论文笔记/1.png" alt=""></p>
<p>如图中所示，我们的算法通过对黑盒学习算法的梯度进行后处理来产生效果。基本的直觉是这样的：我们要确保异常值不会对学习的参数产生较大影响。 从直觉上讲，为了使异常值具有这种效果，其对应的梯度应</p>
<ol>
<li>比较大</li>
<li>系统地指向某一特定方向</li>
</ol>
<p>我们可以通过奇异值分解来检测——如果1和2都成立，那么异常值应该是梯度矩阵中较大的奇异值的原因，这使我们能够检测到并去除它们。</p>
<p>算法1中的伪代码更好的展示了这点。</p>
<p><img src="/images/《SEVER-A-Robust-Meta-Algorithm-for-Stochastic-Optimization》论文笔记/2.png" alt=""></p>
<p><img src="/images/《SEVER-A-Robust-Meta-Algorithm-for-Stochastic-Optimization》论文笔记/3.png" alt=""></p>
<p>具体来说，我们描述该算法是如何在线性回归中发挥作用的。首先，我们会解出数据集上的最佳参数向量，不考虑鲁棒性的问题。</p>
<p>具体来说，令$\hat{w}$为</p>
<script type="math/tex; mode=display">\sum_{i=1}^{n} x_i(\hat{w} \cdot x_i - y_i)=0</script><p>的解，令梯度为0，根据我们的需要给出一个临界点。</p>
<p>我们计算出平均梯度$\frac{1}{n}\sum_{i=1}^{n} x_i(\hat{w} \cdot x_i - y_i)$，并用这个来计算中心梯度矩阵$G$。也就是说，$G$的第$j$行，$G_j$，为向量</p>
<script type="math/tex; mode=display">x_j(\hat{w} \cdot x_j-y_j)-\frac{1}{n}\sum^{n}_{i=1}x_i(\hat{w} \cdot x_i-y_i)</script><p>我们计算出$G$的右上奇异向量，把数据投影到这个方向，然后将所得到的值平方来得出每个点的分数</p>
<script type="math/tex; mode=display">\tau _j = (G_j \cdot v)^2</script><p>有了这些分数后，我们运行算法2，来(随机的)去除那些分数最大的点。我们在这个点的子集上重新运行整个过程，直到算法2不再去除任何一个点。</p>
<h1 id="Theoretical-Guarantees"><a href="#Theoretical-Guarantees" class="headerlink" title="Theoretical Guarantees"></a>Theoretical Guarantees</h1><p>我们的第一个理论结果表明，只要数据不是太重尾(<a href="https://baike.baidu.com/item/重尾分，布/4483429" target="_blank" rel="noopener">重尾分布</a>)，即使存在异常值，SEVER都会找到真实函数$\overline{f}$的近似临界点。</p>
<h2 id="定理2-1"><a href="#定理2-1" class="headerlink" title="定理2.1"></a>定理2.1</h2><p>假设函数$f_1,…,f_n,\overline{f}: H \to \mathbb{R}$被界定在封闭域$H$下，并假设他们都满足如下的确定性规律性条件：</p>
<p>存在集合$I_{good} \subseteq [n]$，其中$|I_{good}| \geq (1 - \varepsilon)n$并且$\sigma&gt;0$</p>
<p>因此：</p>
<ol>
<li><p>$Cov_{I_{good}}[\bigtriangledown f_i(w)] \preceq \sigma^{2}I, w \in H$</p>
</li>
<li><p>$||\bigtriangledown \hat{f}(w) - \bigtriangledown \overline{f}(w)||_{2} \leq \sigma \sqrt{\varepsilon}, w \in H$</p>
<p>其中$\hat{f}\overset{def}{=} \frac{1}{|I_{good}|}\sum_{i \in I_{good}}f_{i}$</p>
</li>
</ol>
<p>这样，我们的算法应用于$f_1,…,f_n,\sigma$返回的是一个点$w \in H$，至少$\frac{9}{10}$的可能性是$\overline{f}$的一个$(\gamma+O(\sigma \sqrt{\varepsilon}))$-近似临界点。</p>
<p>定理2.1的关键点在于，对错误的保证并不依赖于基础维度$d$。相反，大多数自然算法都会随着$d$的增加而增加，因此在高维中的鲁棒性很差。</p>
<p>我们证明了在一些关于$p^{*}$的优美假设下，多项式的样本非常有可能满足确定性规律性条件：</p>
<h2 id="论点2-2-非正式的"><a href="#论点2-2-非正式的" class="headerlink" title="论点2.2 (非正式的)"></a>论点2.2 (非正式的)</h2><p>令$H \in \mathbb{R}^{d}$为一个最大直径为$r$的封闭边界集</p>
<p>$p^{*}$为函数$f: H \to \mathbb{R}$的分布</p>
<p>$\overline{f}=\mathbb{E}_{f \sim p^{*}}[f]$</p>
<p>假设对于任意的$w \in H$和单位向量$v$我们都有</p>
<script type="math/tex; mode=display">\mathbb{E}_{f \sim p^{*}}[(v \cdot (\bigtriangledown{f(w)}-\overline{f}(w)))^{2}] \leq \sigma^2</script><p>在适当的Lipschitz和平滑假设下，对于$n=\Omega(dlog(r/(\sigma^2 \varepsilon))/(\sigma^2 \varepsilon))$，一个从分布$p^{*}$中得到的独立同分布的$\varepsilon$-扰动的函数集合有很大的概率满足条件1和条件2。</p>
<p>由于定理2.1非常笼统，甚至可以用于非凸的损失函数，但我们通常希望不只是一个近似临界点。特别是对于凸问题，我们可以保证能找到一个近似的全局最小值。这是定理2.1的推论：</p>
<h2 id="推论2-3"><a href="#推论2-3" class="headerlink" title="推论2.3"></a>推论2.3</h2><p>假设$f_1,…,f_n: H \to \mathbb{R}$满足规律性条件1和条件2，并且$H$是凸的，具有$l_2$半径$r$。这样，SEVER的输出至少有$\frac{9}{10}$的概率满足如下式子：</p>
<ol>
<li><p>若$\overline{f}$是凸的，算法能够找到一个点$w \in H$满足$\overline{f}(w)-\overline{f}(w^{*})=O((\sigma \sqrt{\varepsilon}+\gamma)r)$ </p>
</li>
<li><p>若$\overline{f}$是$\xi$强凸($\xi$-strongly convex)的，算法能够找到一个点$w \in H$满足$\overline{f}(w)-\overline{f}(w^{*})=O((\varepsilon \sigma^{2}+\gamma^{2})/\xi)$ </p>
</li>
</ol>
<blockquote>
<p>$f$ is $ \xi $-strongly convex if $f(x)-\frac{\xi}{2}x^{T}x$ is convex.</p>
<p>一个函数减去二次函数仍然是凸的，说明它至少有这个二次函数这么凸。或者说这个二次函数是它的一个下界。</p>
<p>参考<a href="https://blog.fangzhou.me/posts/20190217-convex-function-lipschitz-smooth-strongly-convex/" target="_blank" rel="noopener">凸函数，Lipschitz smooth, strongly convex</a></p>
</blockquote>
</div><div class="tags"><a href="/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">论文笔记</a></div><div class="post-nav"><a class="next" href="/2019/11/10/%E5%85%B1%E5%8B%89/">共勉</a></div><div id="vcomment"></div><script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script><script src="//unpkg.com/valine@latest/dist/Valine.min.js"></script><script>var notify = 'false' ? true : false;
var verify = 'false' ? true : false;
var GUEST_INFO = ['nick','mail','link'];
var guest_info = 'nick,mail,link'.split(',').filter(function(item){
  return GUEST_INFO.indexOf(item) > -1
});
guest_info = guest_info.length == 0 ? GUEST_INFO :guest_info;
window.valine = new Valine({
  el:'#vcomment',
  notify:notify,
  verify:verify,
  appId:'OSUK7P1Utb45xOOhM0JJbTab-gzGzoHsz',
  appKey:'0OAn3KMQOEbFWSQB5P17tM7h',
  placeholder:'Just so so',
  avatar:'mm',
  guest_info:guest_info,
  pageSize:'10'
})</script></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank"><input type="text" name="q" maxlength="20" placeholder="Search"/><input type="hidden" name="sitesearch" value="http://hhhhhxh.github.io"/></form></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/%E6%95%99%E7%A8%8B/" style="font-size: 15px;">教程</a> <a href="/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/" style="font-size: 15px;">论文笔记</a> <a href="/tags/%E9%B8%A1%E6%B1%A4/" style="font-size: 15px;">鸡汤</a> <a href="/tags/%E8%B5%84%E6%96%99%E6%95%B4%E7%90%86/" style="font-size: 15px;">资料整理</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2019/11/12/%E3%80%8ASEVER-A-Robust-Meta-Algorithm-for-Stochastic-Optimization%E3%80%8B%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">《SEVER: A Robust Meta-Algorithm for Stochastic Optimization》论文笔记</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/11/10/%E5%85%B1%E5%8B%89/">共勉</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/10/29/git-lfs%E7%9A%84%E4%BD%BF%E7%94%A8/">git lfs的使用</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/10/22/%E9%9B%B6%E6%A0%B7%E6%9C%AC%E5%AD%A6%E4%B9%A0ZSL%E8%B5%84%E6%96%99%E6%95%B4%E7%90%86/">零样本学习ZSL资料整理</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/10/18/%E8%A7%86%E8%A7%89%E9%97%AE%E7%AD%94VQA%E8%B5%84%E6%96%99%E6%95%B4%E7%90%86/">视觉问答VQA资料整理</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/10/07/hello-world/">Hello World</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="http://www.example1.com/" title="site-name1" target="_blank">site-name1</a><ul></ul><a href="http://www.example2.com/" title="site-name2" target="_blank">site-name2</a><ul></ul><a href="http://www.example3.com/" title="site-name3" target="_blank">site-name3</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2019 <a href="/." rel="nofollow">hhhhhxh.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>