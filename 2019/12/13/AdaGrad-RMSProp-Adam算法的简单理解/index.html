<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content=""><title>AdaGrad,RMSProp,Adam算法的简单理解 | hhhhhxh</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/8.0.1/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/3.4.0/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = 'https://hm.baidu.com/hm.js?' + '7fa17d1076a1ae6cf7ec6cc0ad18ad46';
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
  })();</script></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">AdaGrad,RMSProp,Adam算法的简单理解</h1><a id="logo" href="/.">hhhhhxh</a><p class="description"></p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a><a href="/photos/"><i class="fa fa-instagram"> 照片墙</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">AdaGrad,RMSProp,Adam算法的简单理解</h1><div class="post-meta">Dec 13, 2019<script src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> 阅读</span></span></div><a class="disqus-comment-count" href="/2019/12/13/AdaGrad-RMSProp-Adam%E7%AE%97%E6%B3%95%E7%9A%84%E7%AE%80%E5%8D%95%E7%90%86%E8%A7%A3/#vcomment"><span class="valine-comment-count" data-xid="/2019/12/13/AdaGrad-RMSProp-Adam%E7%AE%97%E6%B3%95%E7%9A%84%E7%AE%80%E5%8D%95%E7%90%86%E8%A7%A3/"></span><span> 条评论</span></a><div class="clear"><div class="toc-article" id="toc"><div class="toc-title">文章目录</div><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#前导知识"><span class="toc-number">1.</span> <span class="toc-text">前导知识</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#小批量随机梯度下降"><span class="toc-number">1.1.</span> <span class="toc-text">小批量随机梯度下降</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#指数加权移动平均"><span class="toc-number">1.2.</span> <span class="toc-text">指数加权移动平均</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#AdaGrad"><span class="toc-number">2.</span> <span class="toc-text">AdaGrad</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#RMSProp"><span class="toc-number">3.</span> <span class="toc-text">RMSProp</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Adam"><span class="toc-number">4.</span> <span class="toc-text">Adam</span></a></li></ol></div></div><div class="post-content"><p>假设目标函数$f$的自变量为二维向量$[x_1,x_2]^T$</p>
<p>在之前的算法中，每个元素在迭代时都使用相同的学习率，如：</p>
<script type="math/tex; mode=display">x_1 \larr x_1 - \eta\frac{\partial f}{\partial x_1},x_2 \larr x_2 - \eta\frac{\partial f}{\partial x_2}</script><p>但是这样会使得在梯度较小的维度上迭代过慢，或在梯度较大的维度上发散。因此希望能根据不同梯度值的大小来调整各个维度上的学习率。</p>
<h1 id="前导知识"><a href="#前导知识" class="headerlink" title="前导知识"></a>前导知识</h1><h2 id="小批量随机梯度下降"><a href="#小批量随机梯度下降" class="headerlink" title="小批量随机梯度下降"></a>小批量随机梯度下降</h2><p>在每一个时间步$t$中，小批量随机梯度下降随机均匀采样一个由训练数据样本索引组成的小批量$\mathcal{B}_t$，可以通过重复采样或不重复采样来获得各个样本。</p>
<p>对时间$t$的小批量$\mathcal{B}_t$上目标函数位于$x_{t-1}$处的梯度$g_t$的计算如下：</p>
<script type="math/tex; mode=display">g_t \larr \bigtriangledown f_{\mathcal{B}_t}(x_{t-1})=\frac{1}{|\mathcal{B}|}\sum_{i \in \mathcal{B}_t} \bigtriangledown f_i(x_{t-1})</script><p>其中$|\mathcal{B}|$表示小批量中样本的个数，为超参数。$g_t$为对梯度$\bigtriangledown f(x_{t-1})$的无偏估计。</p>
<p>小批量随机梯度下降对自变量的迭代如下：</p>
<p>$x_t \larr x_{t-1} - \eta_t g_t$</p>
<h2 id="指数加权移动平均"><a href="#指数加权移动平均" class="headerlink" title="指数加权移动平均"></a>指数加权移动平均</h2><p>给定超参数$0 \leq \gamma &lt; 1$，当前时间步$t$的变量$y_t$是上一时间步$t-1$的变量$y_{t-1}$和当前时间步另一变量$x_t$的线性组合：</p>
<script type="math/tex; mode=display">y_t=\gamma y_{t-1}+(1-\gamma) x_t</script><p>对$y_t$展开：</p>
<script type="math/tex; mode=display">y_t=(1-\gamma)x_t+(1- \gamma)\gamma x_{t-1}+(1-\gamma) \gamma^{2} x_{t-2}+\gamma^3 y_{t-3}=\;...</script><p>令$n=1/(1-\gamma)$，则$(1-1/n)^n=\gamma ^{1/(1-\gamma)}$</p>
<p>当$n \rarr \infty$时，$\gamma \rarr 1$，而$lim_{n \rarr \infty}(1-\frac{1}{n})^n=exp(-1)=0.3679…$</p>
<p>可以在近似中忽略所有含$\gamma ^{1/(1-\gamma)}$和比$\gamma ^{1/(1-\gamma)}$更高阶的系数的项。</p>
<p>因此在实际中，常常将$y_t$看作是对最近$1/(1-\gamma)$个时间步的$x_t$值的加权平均，而且离当前时间步$t$越近的$x_t$值获得的权重越大(越接近1)。</p>
<h1 id="AdaGrad"><a href="#AdaGrad" class="headerlink" title="AdaGrad"></a>AdaGrad</h1><p>设定累加变量$s_t$，其累加是小批量随机梯度$g_t$的累加。在时间步0，$s_0$的每个元素初始化为0。</p>
<p>在时间步$t$，首先将$g_t$累加到变量$s_t$：</p>
<script type="math/tex; mode=display">s_t \larr s_{t-1}+g_t \bigodot g_t</script><p>调整目标函数自变量中每个元素的学习率：</p>
<script type="math/tex; mode=display">x_t \larr x_{t-1}-\frac{\eta}{\sqrt{s_t+\epsilon}} \bigodot g_t</script><p>其中，$\bigodot$表示按元素相乘。</p>
<h1 id="RMSProp"><a href="#RMSProp" class="headerlink" title="RMSProp"></a>RMSProp</h1><p>与AdaGrad算法唯一的不同是，$s_t$不是截止至时间$t$所有小批量随机梯度$g_t$按元素平方和，而是将这些梯度按元素平方做指数加权移动平均。给定超参数$0 \leq \gamma &lt; 1$，在时间步$t$：</p>
<script type="math/tex; mode=display">s_t \larr \gamma s_{t-1}+(1- \gamma)g_t \bigodot g_t</script><p>其他步骤与AdaGrad算法相同。</p>
<h1 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h1><p>Adam算法使用了动量变量$v_t$。</p>
<p>给定超参数$0 \leq \beta_1&lt;1，0 \leq \beta_2&lt;1$，在时间步$t$，$v_t$为$g_t$的指数加权移动平均：</p>
<script type="math/tex; mode=display">v_t \larr \beta_1 v_{t-1}+(1-\beta_1)g_t</script><p>再将$g_t \bigodot g_t$做指数加权移动平均得到$s_t$：</p>
<script type="math/tex; mode=display">s_t \larr \beta_2 s_{t-1}+(1- \beta_2)g_t \bigodot g_t</script><p>考虑$v_t$这个式子：</p>
<script type="math/tex; mode=display">v_t=\beta_t v_{t-1}+(1-\beta_1)g_t</script><script type="math/tex; mode=display">\beta_1 v_{t-1}=\beta_1^2 v_{t-2}+(1-\beta_1)\beta_1 g_{t-1}$$（两边乘以$\beta_1$）

$$\beta_1^2 v_{t-2}=\beta_1^3 v_{t-3}+(1-\beta_1)\beta_1^2 g_{t-2}$$（两边乘以$\beta_1^2$）

$$...</script><script type="math/tex; mode=display">\beta_1^{t-1} v_1=\beta_1^{t}v_0+(1-\beta_1) \beta_1^{t-1}g_1$$（两边乘以$\beta_1^{t-1}$）

$v_0=0$，相加后消项可得：$v_t=(1-\beta_1) \sum_{i=1}^{t}\beta_1^{t-i}g_i$

而过去各时间步小批量随机梯度的权值相加为$(1-\beta_1) \sum_{i=1}^{t}\beta_1^{t-i}=1-\beta_1^{t}$

当$t$较小时，上式会比较小，为了消除影响，我们可以将$v_t$再除以$1-\beta_1^t$，从而使权值之和为1，这被叫作**偏差修正**。

对$v_t$和$s_t$作偏差修正：

$$\hat{v_t} \larr \frac{v_t}{1-\beta_1^{t}}</script><script type="math/tex; mode=display">\hat{s_t} \larr \frac{s_t}{1-\beta_2^{t}}</script><p>再调整模型参数中每个元素的学习率：</p>
<script type="math/tex; mode=display">g_t' \larr \frac{\eta \hat{v_t}}{\sqrt{\hat{s_t}}+\epsilon}</script><script type="math/tex; mode=display">x_t \larr x_{t-1}-g_t'</script></div><div class="tags"><a href="/tags/tutorial/">tutorial</a></div><div class="post-nav"><a class="next" href="/2019/11/27/%E5%AF%B9%E9%9B%B6%E6%A0%B7%E6%9C%AC%E5%AD%A6%E4%B9%A0%E4%B8%AD%E6%BD%9C%E5%9C%A8%E7%89%B9%E5%BE%81%E7%9A%84%E5%88%A4%E5%88%AB%E5%AD%A6%E4%B9%A0/">对零样本学习中潜在特征的判别学习</a></div><div id="vcomment"></div><script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script><script src="//unpkg.com/valine@latest/dist/Valine.min.js"></script><script>var notify = 'false' ? true : false;
var verify = 'false' ? true : false;
var GUEST_INFO = ['nick','mail','link'];
var guest_info = 'nick,mail,link'.split(',').filter(function(item){
  return GUEST_INFO.indexOf(item) > -1
});
guest_info = guest_info.length == 0 ? GUEST_INFO :guest_info;
window.valine = new Valine({
  el:'#vcomment',
  notify:notify,
  verify:verify,
  appId:'OSUK7P1Utb45xOOhM0JJbTab-gzGzoHsz',
  appKey:'0OAn3KMQOEbFWSQB5P17tM7h',
  placeholder:'Just so so',
  avatar:'mm',
  guest_info:guest_info,
  pageSize:'10'
})</script></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank"><input type="text" name="q" maxlength="20" placeholder="Search"/><input type="hidden" name="sitesearch" value="http://hhhhhxh.github.io"/></form></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/" style="font-size: 15px;">论文笔记</a> <a href="/tags/%E6%95%99%E7%A8%8B/" style="font-size: 15px;">教程</a> <a href="/tags/%E9%B8%A1%E6%B1%A4/" style="font-size: 15px;">鸡汤</a> <a href="/tags/tutorial/" style="font-size: 15px;">tutorial</a> <a href="/tags/%E8%B5%84%E6%96%99%E6%95%B4%E7%90%86/" style="font-size: 15px;">资料整理</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2019/12/13/AdaGrad-RMSProp-Adam%E7%AE%97%E6%B3%95%E7%9A%84%E7%AE%80%E5%8D%95%E7%90%86%E8%A7%A3/">AdaGrad,RMSProp,Adam算法的简单理解</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/11/27/%E5%AF%B9%E9%9B%B6%E6%A0%B7%E6%9C%AC%E5%AD%A6%E4%B9%A0%E4%B8%AD%E6%BD%9C%E5%9C%A8%E7%89%B9%E5%BE%81%E7%9A%84%E5%88%A4%E5%88%AB%E5%AD%A6%E4%B9%A0/">对零样本学习中潜在特征的判别学习</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/11/14/%E4%B8%80%E7%AF%87%E9%9B%B6%E6%A0%B7%E6%9C%AC%E5%AD%A6%E4%B9%A0%E7%9A%84%E7%BB%BC%E8%BF%B0%E2%80%94%E2%80%94%E9%92%88%E5%AF%B9%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E6%8B%86%E5%88%86/">一篇零样本学习的综述——针对数据集的拆分</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/11/12/SEVER%EF%BC%8C%E4%B8%80%E7%A7%8D%E6%A2%AF%E5%BA%A6%E4%BC%98%E5%8C%96%E7%9A%84%E9%B2%81%E6%A3%92%E5%85%83%E7%AE%97%E6%B3%95/">SEVER——一种梯度优化的鲁棒元算法</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/11/10/%E5%85%B1%E5%8B%89/">共勉</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/10/29/git-lfs%E7%9A%84%E4%BD%BF%E7%94%A8/">git lfs的使用</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/10/22/%E9%9B%B6%E6%A0%B7%E6%9C%AC%E5%AD%A6%E4%B9%A0ZSL%E8%B5%84%E6%96%99%E6%95%B4%E7%90%86/">零样本学习ZSL资料整理</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/10/18/%E8%A7%86%E8%A7%89%E9%97%AE%E7%AD%94VQA%E8%B5%84%E6%96%99%E6%95%B4%E7%90%86/">视觉问答VQA资料整理</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/10/07/hello-world/">Hello World</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="http://www.example1.com/" title="site-name1" target="_blank">site-name1</a><ul></ul><a href="http://www.example2.com/" title="site-name2" target="_blank">site-name2</a><ul></ul><a href="http://www.example3.com/" title="site-name3" target="_blank">site-name3</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2019 <a href="/." rel="nofollow">hhhhhxh.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>